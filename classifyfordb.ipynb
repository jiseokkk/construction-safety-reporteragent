{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e8ea405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥ ì™„ë£Œ: /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/data_md/all_md_titles.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# md íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë”\n",
    "md_dir = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/data_md\"\n",
    "\n",
    "# ì €ì¥í•  íŒŒì¼ ê²½ë¡œ\n",
    "output_path = os.path.join(md_dir, \"all_md_titles.md\")\n",
    "\n",
    "titles = []\n",
    "\n",
    "# md íŒŒì¼ë“¤ ìˆœíšŒ\n",
    "for fname in sorted(os.listdir(md_dir)):\n",
    "    if fname.lower().endswith(\".md\"):\n",
    "        path = os.path.join(md_dir, fname)\n",
    "        \n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ Markdown ì œëª©(#) ì°¾ê¸°\n",
    "        title = None\n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"#\"):\n",
    "                title = line.strip().lstrip(\"#\").strip()\n",
    "                break\n",
    "        \n",
    "        # ì œëª© ì €ì¥\n",
    "        if title:\n",
    "            titles.append(title)\n",
    "\n",
    "# ChatGPTì—ê²Œ ë¶™ì—¬ë„£ê¸° ì¢‹ì€ Markdown êµ¬ì¡° ë§Œë“¤ê¸°\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# ğŸ“„ Markdown íŒŒì¼ ì œëª© ëª©ë¡\\n\\n\")\n",
    "    for i, t in enumerate(titles, 1):\n",
    "        f.write(f\"{i}. {t}\\n\")\n",
    "\n",
    "print(\"ì €ì¥ ì™„ë£Œ:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19a6f1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ë¯¸ë¶„ë¥˜] í•´ì²´ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ë°€íê³µê°„ì˜ ë°©ìˆ˜ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[ë¯¸ë¶„ë¥˜] ì² ê³¨ê³µì‚¬ ë¬´ì§€ë³´ ê±°í‘¸ì§‘ë™ë°”ë¦¬(ë°í¬í”Œë ˆì´íŠ¸ ê³µë²•)ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ë¯¸ì¥ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[ë¯¸ë¶„ë¥˜] ì² íƒ‘ê³µì‚¬ ì•ˆì „ë³´ê±´ê¸°ìˆ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[ë¯¸ë¶„ë¥˜] ê±´ì„¤ê³µì‚¬ì˜ ê³ ì†Œì‘ì—…ëŒ€ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] íƒ€ì›Œí¬ë ˆì¸ ì„¤ì¹˜, ì¡°ë¦½, í•´ì²´ ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(Soil Nailing ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ì‚¬ì¥êµ êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[ë¯¸ë¶„ë¥˜] ì¡°ì ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] í„°ë„ê³µì‚¬(NTRê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] êµëŸ‰ê³µì‚¬ì˜ ì´ë™ì‹ ë¹„ê³„ê³µë²•(MSS) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] ì½˜í¬ë¦¬íŠ¸ê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(ì§€í•˜ì—°ì†ë²½) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ê°•ì•„ì¹˜êµ(ë²¤íŠ¸ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] ìš°ë¬¼í†µê¸°ì´ˆ ì•ˆì „ë³´ê±´ ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ê°€ì„¤êµ¬ì¡°ë¬¼ì˜ ì„¤ê³„ë³€ê²½ ìš”ì²­ ë‚´ìš©, ì ˆì°¨ ë“±ì— ê´€í•œ ì‘ì„±ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[ë¯¸ë¶„ë¥˜] ì•¼ê°„ ê±´ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ì‹œíŠ¸(Sheet)ë°©ìˆ˜ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(ê°•ë„ë§ëš, Sheet Pile)ì˜ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[ë¯¸ë¶„ë¥˜] ê°±í¼(Gang form) ì œì‘ ë° ì‚¬ìš©ì•ˆì „ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[ë¯¸ë¶„ë¥˜] ì¤‘ì†Œê·œëª¨ ê±´ì„¤ì—…ì²´ ë³¸ì‚¬ì˜ ì•ˆì „ë³´ê±´ê´€ë¦¬ì— ê´€í•œ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ë‚™í•˜ë¬¼ ë°©í˜¸ì„ ë°˜ ì„¤ì¹˜ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(Earth Anchor ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] í„°ë„ê³µì‚¬(í”„ë¡ íŠ¸ì­í‚¹) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(ì—„ì§€ë§ëš ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ê±´ì„¤ê³µì‚¬ êµ´ì°©ë©´ ì•ˆì „ê¸°ìš¸ê¸° ê¸°ì¤€ì— ê´€í•œ ê¸°ìˆ ì§€ì¹¨.md â†’ 02_earth\n",
      "[ë¯¸ë¶„ë¥˜] ì‹œìŠ¤í…œí¼(RCSí¼,ACSí¼ ì¤‘ì‹¬) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ê±´ì¶•ë¬¼ì˜ ì„ê³µì‚¬(ë‚´ì™¸ì¥) ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ì´ë™ì‹ í¬ë ˆì¸ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] PCTê±°ë” êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] ê°•ê´€ë¹„ê³„ ì•ˆì „ì‘ì—…ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[ë¯¸ë¶„ë¥˜] ê±´ì„¤í˜„ì¥ ìš©ì ‘ìš©ë‹¨ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[ë¯¸ë¶„ë¥˜] ì¡°ê²½ê³µì‚¬(ìˆ˜ëª©ì‹ì¬ì‘ì—…) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ìˆ˜ì§ë³´í˜¸ë§ ì„¤ì¹˜ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ëƒ‰ë™ëƒ‰ì¥ ë¬¼ë¥˜ì°½ê³  ë‹¨ì—´ê³µì‚¬ í™”ì¬ì˜ˆë°© ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] í˜„ìˆ˜êµ êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] í•­íƒ€ê¸°, í•­ë°œê¸° ì‚¬ìš© ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì¹¨.md â†’ 05_crane\n",
      "[ë¯¸ë¶„ë¥˜] ìˆ˜ìƒ ë°”ì§€(Barge)ì„  ì´ìš© ê±´ì„¤ê³µì‚¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] í„°ë„ê³µì‚¬(Shield-T.B.Mê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] ì‘ì—…ë°œíŒ ì„¤ì¹˜ ë° ì‚¬ìš©ì•ˆì „ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] êµëŸ‰ ìŠ¬ë˜ë¸Œê±°í‘¸ì§‘ í•´ì²´ìš© ì‘ì—…ëŒ€ì°¨ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(ë ì¥ê¸´ì¥ê³µë²•, Prestressed Wale Method) ì•ˆì „ë³´ê±´ ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[ë¯¸ë¶„ë¥˜] í™”í•™í”ŒëœíŠ¸ ê°œë³´ìˆ˜ ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ê±´ì„¤í˜„ì¥ì˜ ì¤‘ëŸ‰ë¬¼ ì·¨ê¸‰ ì‘ì—…ê³„íšì„œ(ì´ë™ì‹í¬ë ˆì¸) ì‘ì„±ì§€ì¹¨.md â†’ 05_crane\n",
      "[ë¯¸ë¶„ë¥˜] ì´ˆê³ ì¸µ ê±´ì¶•ë¬¼ê³µì‚¬(í™”ì¬ì˜ˆë°©) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[ë¯¸ë¶„ë¥˜] ê³¤ëŒë¼(Gondola) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] êµëŸ‰ê³µì‚¬(ë¼ë©˜êµ) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] ë°œíŒŒê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] ì§€í•˜ë§¤ì„¤ë¬¼ êµ´ì°©ê³µì‚¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ë‚´ì¥ê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ì´ë™ì‹ ë¹„ê³„ ì„¤ì¹˜ ë° ì‚¬ìš©ì•ˆì „ ê¸°ìˆ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[ë¯¸ë¶„ë¥˜] ê±´ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ ì„¤ê³„ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] í˜„ìˆ˜êµ ì£¼íƒ‘ì‹œê³µ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] êµ´ì°©ê³µì‚¬ ê³„ì¸¡ê´€ë¦¬ ê¸°ìˆ ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ì˜¹ë²½(ì½˜í¬ë¦¬íŠ¸ ì˜¹ë²½)ê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[ë¯¸ë¶„ë¥˜] ìŠ¬ë¦½í¼(Slip form) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[ë¯¸ë¶„ë¥˜] ì¤‘ì†Œê·œëª¨ ê´€ë¡œê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[ë¯¸ë¶„ë¥˜] ì•ˆì „ëŒ€ ì‚¬ìš©ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ë¤í”„íŠ¸ëŸ­ ë° í™”ë¬¼ìë™ì°¨ ì•ˆì „ì‘ì—…ì§€ì¹¨.md â†’ 05_crane\n",
      "[ë¯¸ë¶„ë¥˜] ì´ˆê³ ì¸µ ê±´ì¶•ë¬¼ê³µì‚¬(ì¼ë°˜ì‚¬í•­) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ê¸ˆì† ì»¤íŠ¼ì›”(Curtain wall) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ì‘ì—…ì˜ìí˜• ë‹¬ë¹„ê³„ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] íŠ¸ëŸ­ íƒ‘ì¬í˜• í¬ë ˆì¸(Cago crane) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] ê°€ê³µì†¡ì „ì„ ë¡œ ì² íƒ‘ ì‹¬í˜•ê¸°ì´ˆê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] í™ë§‰ì´ê³µì‚¬ (SCW ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ìˆ˜ì§í˜• ì¶”ë½ë°©ë§ ì„¤ì¹˜ ê¸°ìˆ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ê°€ì„¤ê³„ë‹¨ ì„¤ì¹˜ ë° ì‚¬ìš© ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ì†Œê·œëª¨ ì² ê·¼ì½˜í¬ë¦¬íŠ¸ êµëŸ‰ê³µì‚¬ ê±°í‘¸ì§‘ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[ë¯¸ë¶„ë¥˜] all_md_titles.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] êµ´ì°©ê¸° ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[ë¯¸ë¶„ë¥˜] íŒŒì´í”„ ì„œí¬íŠ¸ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ê¸°ì„± ì½˜í¬ë¦¬íŠ¸ íŒŒì¼ í•­íƒ€ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[ë¯¸ë¶„ë¥˜] ì² ê³¨ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(C.I.Pê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ì‹œìŠ¤í…œ ë¹„ê³„ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] I.L.M êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] íŠ¸ëŸ¬ìŠ¤ê±°ë” êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] í•´ìƒ RCD í˜„ì¥íƒ€ì„¤ ë§ëšê³µì‚¬(í˜„ìˆ˜êµ, ì‚¬ì¥êµ) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] êµ´ì°©ê³µì‚¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[ë¯¸ë¶„ë¥˜] ì·¨ì•½ì‹œê¸° ê±´ì„¤í˜„ì¥ ì•ˆì „ì‘ì—…ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ê±´ì„¤ê¸°ê³„ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 05_crane\n",
      "[ë¯¸ë¶„ë¥˜] ê´€ë¡œë§¤ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] í„°ë„ê³µì‚¬(ì¹¨ë§¤ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] ë¸”ë¡ì‹ ë³´ê°•í†  ì˜¹ë²½ ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[ë¯¸ë¶„ë¥˜] ê´€ë¡œë§¤ì„¤ê³µì‚¬(ìœ ì••ì‹ ì¶”ì§„ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] ë‚™í•˜ë¬¼ ë°©ì§€ë§ ì„¤ì¹˜ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] íƒ€ì¼(Tile) ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] êµëŸ‰ê³µì‚¬(P.S.Mê³µë²•) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[ë¯¸ë¶„ë¥˜] ì¶”ë½ë°©í˜¸ë§ ì„¤ì¹˜ ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] í”„ë¦¬ìºìŠ¤íŠ¸ ì½˜í¬ë¦¬íŠ¸ ê±´ì¶•êµ¬ì¡°ë¬¼ ì¡°ë¦½ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[OK] í”„ë¦¬ìŠ¤íŠ¸ë ˆìŠ¤íŠ¸ ì½˜í¬ë¦¬íŠ¸(PSC) êµëŸ‰ê³µì‚¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[ë¯¸ë¶„ë¥˜] ê±´ì„¤ê³µì‚¬ ëŒê´€ì‘ì—… ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] F.C.M êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] íƒ‘ë‹¤ìš´(Top down) ê³µë²• ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] ë‹¨ìˆœ ìŠ¬ë˜ë¸Œ ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[OK] ì•„ìŠ¤íŒ”íŠ¸ì½˜í¬ë¦¬íŠ¸ í¬ì¥ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[ë¯¸ë¶„ë¥˜] ì‹œìŠ¤í…œ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\n",
      "[OK] í„°ë„ê³µì‚¬(NATMê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] ê²½ëŸ‰ì² ê³¨ ì²œì¥ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "\n",
      "ğŸ‰ DB íŒŒì¼ ë¶„ë¥˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# md íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬\n",
    "md_dir = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/data_md\"\n",
    "\n",
    "# DB í´ë” ê¸°ë³¸ê²½ë¡œ\n",
    "db_root = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/DB\"\n",
    "\n",
    "# ====== 7ê°œ DB ë¶„ë¥˜ ê¸°ì¤€ ======\n",
    "DB_RULES = {\n",
    "    \"01_bridge\": [\n",
    "        \"êµëŸ‰\", \"ê±°ë”\", \"ì•„ì¹˜êµ\", \"ì‚¬ì¥êµ\", \"í˜„ìˆ˜êµ\", \"PSM\", \"RCD\", \"íŠ¸ëŸ¬ìŠ¤\", \"MSS\", \"ë¼ë©˜êµ\"\n",
    "    ],\n",
    "    \"02_earth\": [\n",
    "        \"í™ë§‰ì´\", \"êµ´ì°©\", \"CIP\", \"SCW\", \"Earth Anchor\", \"Soil Nailing\",\n",
    "        \"Sheet Pile\", \"ì—„ì§€ë§ëš\", \"ì—°ì†ë²½\", \"ì˜¹ë²½\", \"ë³´ê°•í† \", \"ê¸°ì´ˆ\"\n",
    "    ],\n",
    "    \"03_tunnel\": [\n",
    "        \"í„°ë„\", \"NATM\", \"NTR\", \"TBM\", \"Shield\", \"ì¹¨ë§¤ê³µë²•\", \"ë°œíŒŒ\", \"íƒ‘ë‹¤ìš´\"\n",
    "    ],\n",
    "    \"04_scaffold\": [\n",
    "        \"ë¹„ê³„\", \"ê°€ì„¤\", \"ì‘ì—…ë°œíŒ\", \"ë°©ì§€ë§\", \"ë°©í˜¸ì„ ë°˜\", \"ë³´í˜¸ë§\",\n",
    "        \"ë‹¬ë¹„ê³„\", \"ìˆ˜ì§ë³´í˜¸ë§\", \"ì¶”ë½ë°©ë§\", \"ê°€ì„¤ê³„ë‹¨\"\n",
    "    ],\n",
    "    \"05_crane\": [\n",
    "        \"í¬ë ˆì¸\", \"ì–‘ì¤‘\", \"ê±´ì„¤ê¸°ê³„\", \"ì¤‘ëŸ‰ë¬¼\", \"íƒ€ì›Œí¬ë ˆì¸\",\n",
    "        \"í•­íƒ€ê¸°\", \"í•­ë°œê¸°\", \"ë¤í”„íŠ¸ëŸ­\", \"í™”ë¬¼ìë™ì°¨\", \"ë°”ì§€ì„ \"\n",
    "    ],\n",
    "    \"06_finishing\": [\n",
    "        \"ì„ê³µì‚¬\", \"ì»¤íŠ¼ì›”\", \"ë‚´ì¥\", \"ë¯¸ì¥\", \"íƒ€ì¼\", \"ë‹¨ì—´\", \"ë°©ìˆ˜\", \"ì²œì¥\"\n",
    "    ],\n",
    "    \"07_concrete\": [\n",
    "        \"ì½˜í¬ë¦¬íŠ¸\", \"ìŠ¬ë˜ë¸Œ\", \"íŒŒì¼í•­íƒ€\", \"í”„ë¦¬ìºìŠ¤íŠ¸\", \"PSC\", \"PC\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ====== DB í´ë” ìƒì„± ======\n",
    "os.makedirs(db_root, exist_ok=True)\n",
    "for db_name in DB_RULES.keys():\n",
    "    os.makedirs(os.path.join(db_root, db_name), exist_ok=True)\n",
    "\n",
    "# ====== íŒŒì¼ ë¶„ë¥˜ í•¨ìˆ˜ ======\n",
    "def classify_md(title):\n",
    "    for db_name, keywords in DB_RULES.items():\n",
    "        for kw in keywords:\n",
    "            if kw in title:\n",
    "                return db_name\n",
    "    return None  # ë¯¸ë¶„ë¥˜\n",
    "\n",
    "# ====== md íŒŒì¼ ì½ê³  ë¶„ë¥˜ í›„ ì´ë™ ======\n",
    "for fname in os.listdir(md_dir):\n",
    "    if not fname.endswith(\".md\"):\n",
    "        continue\n",
    "\n",
    "    src_path = os.path.join(md_dir, fname)\n",
    "\n",
    "    with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first_line = f.readline().strip()\n",
    "\n",
    "    # ì œëª© ì •ì œ\n",
    "    title = first_line.lstrip(\"# \").strip()\n",
    "\n",
    "    db_category = classify_md(title)\n",
    "\n",
    "    if db_category is None:\n",
    "        print(f\"[ë¯¸ë¶„ë¥˜] {fname} â†’ ë¶„ë¥˜ ê·œì¹™ì— ë§ì§€ ì•ŠìŒ\")\n",
    "        continue\n",
    "\n",
    "    dst_path = os.path.join(db_root, db_category, fname)\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n",
    "    print(f\"[OK] {fname} â†’ {db_category}\")\n",
    "\n",
    "print(\"\\nğŸ‰ DB íŒŒì¼ ë¶„ë¥˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03db70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== ğŸ“Œ MD íŒŒì¼ ë¶„ë¥˜ ê²€ì¦ ê²°ê³¼ =====\n",
      "\n",
      "ğŸ“„ ì›ë³¸ md íŒŒì¼ ê°œìˆ˜: 100\n",
      "ğŸ“ DBì— ë¶„ë¥˜ëœ md íŒŒì¼ ê°œìˆ˜: 71\n",
      "\n",
      "âŒ ëˆ„ë½ëœ md íŒŒì¼:\n",
      "  - all_md_titles.md\n",
      "  - ê°±í¼(Gang form) ì œì‘ ë° ì‚¬ìš©ì•ˆì „ ì§€ì¹¨.md\n",
      "  - ê±´ì„¤ê³µì‚¬ ëŒê´€ì‘ì—… ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md\n",
      "  - ê±´ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ ì„¤ê³„ ì§€ì¹¨.md\n",
      "  - ê±´ì„¤ê³µì‚¬ì˜ ê³ ì†Œì‘ì—…ëŒ€ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md\n",
      "  - ê±´ì„¤í˜„ì¥ ìš©ì ‘ìš©ë‹¨ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md\n",
      "  - ê³¤ëŒë¼(Gondola) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md\n",
      "  - ê´€ë¡œë§¤ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md\n",
      "  - ê´€ë¡œë§¤ì„¤ê³µì‚¬(ìœ ì••ì‹ ì¶”ì§„ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md\n",
      "  - ìˆ˜ìƒ ë°”ì§€(Barge)ì„  ì´ìš© ê±´ì„¤ê³µì‚¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md\n",
      "  - ìŠ¬ë¦½í¼(Slip form) ì•ˆì „ì‘ì—… ì§€ì¹¨.md\n",
      "  - ì‹œìŠ¤í…œ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md\n",
      "  - ì‹œìŠ¤í…œí¼(RCSí¼,ACSí¼ ì¤‘ì‹¬) ì•ˆì „ì‘ì—… ì§€ì¹¨.md\n",
      "  - ì•ˆì „ëŒ€ ì‚¬ìš©ì§€ì¹¨.md\n",
      "  - ì•¼ê°„ ê±´ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md\n",
      "  - ì¡°ê²½ê³µì‚¬(ìˆ˜ëª©ì‹ì¬ì‘ì—…) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md\n",
      "  - ì¡°ì ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md\n",
      "  - ì¤‘ì†Œê·œëª¨ ê±´ì„¤ì—…ì²´ ë³¸ì‚¬ì˜ ì•ˆì „ë³´ê±´ê´€ë¦¬ì— ê´€í•œ ì§€ì¹¨.md\n",
      "  - ì¤‘ì†Œê·œëª¨ ê´€ë¡œê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md\n",
      "  - ì² ê³¨ê³µì‚¬ ë¬´ì§€ë³´ ê±°í‘¸ì§‘ë™ë°”ë¦¬(ë°í¬í”Œë ˆì´íŠ¸ ê³µë²•)ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md\n",
      "  - ì² ê³¨ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md\n",
      "  - ì² íƒ‘ê³µì‚¬ ì•ˆì „ë³´ê±´ê¸°ìˆ ì§€ì¹¨.md\n",
      "  - ì´ˆê³ ì¸µ ê±´ì¶•ë¬¼ê³µì‚¬(ì¼ë°˜ì‚¬í•­) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md\n",
      "  - ì´ˆê³ ì¸µ ê±´ì¶•ë¬¼ê³µì‚¬(í™”ì¬ì˜ˆë°©) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md\n",
      "  - ì¶”ë½ë°©í˜¸ë§ ì„¤ì¹˜ ì§€ì¹¨.md\n",
      "  - ì·¨ì•½ì‹œê¸° ê±´ì„¤í˜„ì¥ ì•ˆì „ì‘ì—…ì§€ì¹¨.md\n",
      "  - íŒŒì´í”„ ì„œí¬íŠ¸ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md\n",
      "  - í•´ì²´ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md\n",
      "  - í™”í•™í”ŒëœíŠ¸ ê°œë³´ìˆ˜ ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md\n",
      "\n",
      "â­• DB í´ë”ì—ë§Œ ì¡´ì¬í•˜ëŠ” ì˜ì‹¬ íŒŒì¼ ì—†ìŒ\n",
      "\n",
      "â­• ì¤‘ë³µ ë¶„ë¥˜ëœ íŒŒì¼ ì—†ìŒ\n",
      "\n",
      "===== ì™„ë£Œ =====\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "md_dir = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/data_md\"\n",
    "db_root = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/DB\"\n",
    "\n",
    "# Step 1. ì›ë³¸ md íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘\n",
    "original_files = sorted([f for f in os.listdir(md_dir) if f.endswith(\".md\")])\n",
    "\n",
    "# Step 2. ëª¨ë“  DB í´ë”ì—ì„œ íŒŒì¼ ìˆ˜ì§‘\n",
    "db_files = []\n",
    "db_map = {}  # íŒŒì¼ â†’ DB í´ë”\n",
    "\n",
    "for db_name in sorted(os.listdir(db_root)):\n",
    "    db_path = os.path.join(db_root, db_name)\n",
    "    if not os.path.isdir(db_path):\n",
    "        continue\n",
    "    for f in os.listdir(db_path):\n",
    "        if f.endswith(\".md\"):\n",
    "            db_files.append(f)\n",
    "            db_map[f] = db_name\n",
    "\n",
    "# Step 3. ë¹„êµ\n",
    "original_set = set(original_files)\n",
    "db_set = set(db_files)\n",
    "\n",
    "missing = original_set - db_set       # ëˆ„ë½ëœ íŒŒì¼\n",
    "extra = db_set - original_set         # ì´ìƒí•˜ê²Œ DBì—ë§Œ ì¡´ì¬í•˜ëŠ” íŒŒì¼\n",
    "duplicates = [f for f in db_files if db_files.count(f) > 1]\n",
    "\n",
    "print(\"===== ğŸ“Œ MD íŒŒì¼ ë¶„ë¥˜ ê²€ì¦ ê²°ê³¼ =====\\n\")\n",
    "print(f\"ğŸ“„ ì›ë³¸ md íŒŒì¼ ê°œìˆ˜: {len(original_files)}\")\n",
    "print(f\"ğŸ“ DBì— ë¶„ë¥˜ëœ md íŒŒì¼ ê°œìˆ˜: {len(db_files)}\\n\")\n",
    "\n",
    "# ëˆ„ë½ íŒŒì¼\n",
    "if missing:\n",
    "    print(\"âŒ ëˆ„ë½ëœ md íŒŒì¼:\")\n",
    "    for m in sorted(missing):\n",
    "        print(\"  -\", m)\n",
    "else:\n",
    "    print(\"â­• ëˆ„ë½ëœ íŒŒì¼ ì—†ìŒ\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ì´ìƒ(ë¶ˆí•„ìš”í•˜ê²Œ DBì—ë§Œ ìˆëŠ” íŒŒì¼)\n",
    "if extra:\n",
    "    print(\"âš ï¸ DB í´ë”ì—ë§Œ ì¡´ì¬í•˜ëŠ” ì˜ì‹¬ íŒŒì¼:\")\n",
    "    for e in sorted(extra):\n",
    "        print(\"  -\", e)\n",
    "else:\n",
    "    print(\"â­• DB í´ë”ì—ë§Œ ì¡´ì¬í•˜ëŠ” ì˜ì‹¬ íŒŒì¼ ì—†ìŒ\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ì¤‘ë³µ íŒŒì¼ (ë‘ DBì— ë“¤ì–´ê°€ ìˆëŠ” ê²½ìš°)\n",
    "if duplicates:\n",
    "    print(\"âš ï¸ ì¤‘ë³µ ë¶„ë¥˜ëœ íŒŒì¼:\")\n",
    "    seen = set()\n",
    "    for d in duplicates:\n",
    "        if d not in seen:\n",
    "            print(f\"  - {d} (DB: {db_map.get(d)})\")\n",
    "            seen.add(d)\n",
    "else:\n",
    "    print(\"â­• ì¤‘ë³µ ë¶„ë¥˜ëœ íŒŒì¼ ì—†ìŒ\")\n",
    "\n",
    "print(\"\\n===== ì™„ë£Œ =====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac79154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Jupyter ì¶œë ¥ ì œí•œ í•´ì œ\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "\n",
    "# Python ê¸°ë³¸ ì¶œë ¥ ì œí•œë„ í•´ì œ\n",
    "sys.setrecursionlimit(1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6eb4f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ ë¯¸ë¶„ë¥˜ ëª©ë¡ ì €ì¥ ì™„ë£Œ!\n",
      "ì €ì¥ ìœ„ì¹˜ â†’ /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/unclassified_titles.txt\n",
      "ì´ 28ê°œ íŒŒì¼ ë¯¸ë¶„ë¥˜ë¨\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# md íŒŒì¼ ìœ„ì¹˜\n",
    "md_dir = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/data_md\"\n",
    "\n",
    "# DB ë¶„ë¥˜ ê·œì¹™ (ê¸°ì¡´)\n",
    "DB_RULES = {\n",
    "    \"01_bridge\": [\"êµëŸ‰\", \"ê±°ë”\", \"ì•„ì¹˜êµ\", \"ì‚¬ì¥êµ\", \"í˜„ìˆ˜êµ\", \"PSM\", \"íŠ¸ëŸ¬ìŠ¤\", \"RCD\", \"ë¼ë©˜\"],\n",
    "    \"02_earth\": [\"í™ë§‰ì´\", \"êµ´ì°©\", \"ì§€í•˜ë§¤ì„¤\", \"SCW\", \"CIP\", \"Earth\", \"Nailing\", \"Sheet\", \"ì˜¹ë²½\", \"ë³´ê°•í† \", \"ê¸°ì´ˆ\"],\n",
    "    \"03_tunnel\": [\"í„°ë„\", \"NATM\", \"NTR\", \"TBM\", \"Shield\", \"ì¹¨ë§¤\", \"ë°œíŒŒ\", \"í”„ë¡ íŠ¸ì­í‚¹\"],\n",
    "    \"04_scaffold\": [\"ë¹„ê³„\", \"ê°€ì„¤\", \"ì‘ì—…ë°œíŒ\", \"ë°©í˜¸\", \"ë‚™í•˜ë¬¼\", \"ì¶”ë½\", \"ë³´í˜¸ë§\", \"ë‹¬ë¹„ê³„\", \"ê°€ì„¤ê³„ë‹¨\"],\n",
    "    \"05_crane\": [\"í¬ë ˆì¸\", \"ì–‘ì¤‘\", \"ê±´ì„¤ê¸°ê³„\", \"ì¤‘ëŸ‰ë¬¼\", \"íƒ€ì›Œí¬ë ˆì¸\", \"í•­íƒ€ê¸°\", \"ë¤í”„\", \"íŠ¸ëŸ­\", \"ë°”ì§€ì„ \"],\n",
    "    \"06_finishing\": [\"ì„ê³µì‚¬\", \"ì»¤íŠ¼ì›”\", \"ë‚´ì¥\", \"ë¯¸ì¥\", \"íƒ€ì¼\", \"ë‹¨ì—´\", \"ë°©ìˆ˜\", \"ì²œì¥\"],\n",
    "    \"07_concrete\": [\"ì½˜í¬ë¦¬íŠ¸\", \"ìŠ¬ë˜ë¸Œ\", \"íŒŒì¼\", \"í”„ë¦¬ìºìŠ¤íŠ¸\", \"PSC\", \"PC\"],\n",
    "}\n",
    "\n",
    "def classify(title):\n",
    "    for db_name, keywords in DB_RULES.items():\n",
    "        for kw in keywords:\n",
    "            if kw in title:\n",
    "                return db_name\n",
    "    return None\n",
    "\n",
    "unclassified = []\n",
    "\n",
    "for fname in sorted(os.listdir(md_dir)):\n",
    "    if fname.endswith(\".md\"):\n",
    "        with open(os.path.join(md_dir, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "            first_line = f.readline().strip()\n",
    "            title = first_line.lstrip(\"# \").strip()\n",
    "        \n",
    "        category = classify(title)\n",
    "        if category is None:\n",
    "            unclassified.append((fname, title))\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ\n",
    "output_file = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/unclassified_titles.txt\"\n",
    "\n",
    "# íŒŒì¼ì— ì €ì¥\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for fname, title in unclassified:\n",
    "        f.write(f\"{fname} : {title}\\n\")\n",
    "\n",
    "print(\"ğŸ“„ ë¯¸ë¶„ë¥˜ ëª©ë¡ ì €ì¥ ì™„ë£Œ!\")\n",
    "print(\"ì €ì¥ ìœ„ì¹˜ â†’\", output_file)\n",
    "print(f\"ì´ {len(unclassified)}ê°œ íŒŒì¼ ë¯¸ë¶„ë¥˜ë¨\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66295a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] F.C.M êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] I.L.M êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] PCTê±°ë” êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] ê°€ê³µì†¡ì „ì„ ë¡œ ì² íƒ‘ ì‹¬í˜•ê¸°ì´ˆê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ê°€ì„¤ê³„ë‹¨ ì„¤ì¹˜ ë° ì‚¬ìš© ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ê°€ì„¤êµ¬ì¡°ë¬¼ì˜ ì„¤ê³„ë³€ê²½ ìš”ì²­ ë‚´ìš©, ì ˆì°¨ ë“±ì— ê´€í•œ ì‘ì„±ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ê°•ê´€ë¹„ê³„ ì•ˆì „ì‘ì—…ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ê°•ì•„ì¹˜êµ(ë²¤íŠ¸ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] ê°±í¼(Gang form) ì œì‘ ë° ì‚¬ìš©ì•ˆì „ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ê±´ì„¤ê³µì‚¬ êµ´ì°©ë©´ ì•ˆì „ê¸°ìš¸ê¸° ê¸°ì¤€ì— ê´€í•œ ê¸°ìˆ ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ê±´ì„¤ê³µì‚¬ ëŒê´€ì‘ì—… ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 08_general\n",
      "[âŒ ë¯¸ë¶„ë¥˜] ê±´ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ ì„¤ê³„ ì§€ì¹¨.md : ê±´ì„¤ê³µì‚¬ì•ˆì „Â·ë³´ê±´ì„¤ê³„ì§€ì¹¨\n",
      "[OK] ê±´ì„¤ê³µì‚¬ì˜ ê³ ì†Œì‘ì—…ëŒ€ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] ê±´ì„¤ê¸°ê³„ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] ê±´ì„¤í˜„ì¥ ìš©ì ‘ìš©ë‹¨ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ 08_general\n",
      "[OK] ê±´ì„¤í˜„ì¥ì˜ ì¤‘ëŸ‰ë¬¼ ì·¨ê¸‰ ì‘ì—…ê³„íšì„œ(ì´ë™ì‹í¬ë ˆì¸) ì‘ì„±ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] ê±´ì¶•ë¬¼ì˜ ì„ê³µì‚¬(ë‚´ì™¸ì¥) ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ê²½ëŸ‰ì² ê³¨ ì²œì¥ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ê³¤ëŒë¼(Gondola) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ê´€ë¡œë§¤ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ê´€ë¡œë§¤ì„¤ê³µì‚¬(ìœ ì••ì‹ ì¶”ì§„ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] êµëŸ‰ ìŠ¬ë˜ë¸Œê±°í‘¸ì§‘ í•´ì²´ìš© ì‘ì—…ëŒ€ì°¨ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] êµëŸ‰ê³µì‚¬(P.S.Mê³µë²•) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] êµëŸ‰ê³µì‚¬(ë¼ë©˜êµ) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] êµëŸ‰ê³µì‚¬ì˜ ì´ë™ì‹ ë¹„ê³„ê³µë²•(MSS) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] êµ´ì°©ê³µì‚¬ ê³„ì¸¡ê´€ë¦¬ ê¸°ìˆ ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] êµ´ì°©ê³µì‚¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] êµ´ì°©ê¸° ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ê¸ˆì† ì»¤íŠ¼ì›”(Curtain wall) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ê¸°ì„± ì½˜í¬ë¦¬íŠ¸ íŒŒì¼ í•­íƒ€ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[OK] ë‚™í•˜ë¬¼ ë°©ì§€ë§ ì„¤ì¹˜ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ë‚™í•˜ë¬¼ ë°©í˜¸ì„ ë°˜ ì„¤ì¹˜ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ë‚´ì¥ê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ëƒ‰ë™ëƒ‰ì¥ ë¬¼ë¥˜ì°½ê³  ë‹¨ì—´ê³µì‚¬ í™”ì¬ì˜ˆë°© ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ë‹¨ìˆœ ìŠ¬ë˜ë¸Œ ì½˜í¬ë¦¬íŠ¸ íƒ€ì„¤ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[OK] ë¤í”„íŠ¸ëŸ­ ë° í™”ë¬¼ìë™ì°¨ ì•ˆì „ì‘ì—…ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] ë¯¸ì¥ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ë°€íê³µê°„ì˜ ë°©ìˆ˜ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] ë°œíŒŒê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] ë¸”ë¡ì‹ ë³´ê°•í†  ì˜¹ë²½ ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ì‚¬ì¥êµ êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] ì†Œê·œëª¨ ì² ê·¼ì½˜í¬ë¦¬íŠ¸ êµëŸ‰ê³µì‚¬ ê±°í‘¸ì§‘ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] ìˆ˜ìƒ ë°”ì§€(Barge)ì„  ì´ìš© ê±´ì„¤ê³µì‚¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] ìˆ˜ì§ë³´í˜¸ë§ ì„¤ì¹˜ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ìˆ˜ì§í˜• ì¶”ë½ë°©ë§ ì„¤ì¹˜ ê¸°ìˆ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ìŠ¬ë¦½í¼(Slip form) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ì‹œìŠ¤í…œ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ì‹œìŠ¤í…œ ë¹„ê³„ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ì‹œìŠ¤í…œí¼(RCSí¼,ACSí¼ ì¤‘ì‹¬) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ì‹œíŠ¸(Sheet)ë°©ìˆ˜ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ì•„ìŠ¤íŒ”íŠ¸ì½˜í¬ë¦¬íŠ¸ í¬ì¥ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[OK] ì•ˆì „ëŒ€ ì‚¬ìš©ì§€ì¹¨.md â†’ 08_general\n",
      "[OK] ì•¼ê°„ ê±´ì„¤ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 08_general\n",
      "[OK] ì˜¹ë²½(ì½˜í¬ë¦¬íŠ¸ ì˜¹ë²½)ê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ìš°ë¬¼í†µê¸°ì´ˆ ì•ˆì „ë³´ê±´ ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ì´ë™ì‹ ë¹„ê³„ ì„¤ì¹˜ ë° ì‚¬ìš©ì•ˆì „ ê¸°ìˆ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ì´ë™ì‹ í¬ë ˆì¸ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] ì‘ì—…ë°œíŒ ì„¤ì¹˜ ë° ì‚¬ìš©ì•ˆì „ ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ì‘ì—…ì˜ìí˜• ë‹¬ë¹„ê³„ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] ì¡°ê²½ê³µì‚¬(ìˆ˜ëª©ì‹ì¬ì‘ì—…) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 06_finishing\n",
      "[âŒ ë¯¸ë¶„ë¥˜] ì¡°ì ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md : ì¡°ì ê³µì‚¬ì•ˆì „ë³´ê±´ì‘ì—…ê¸°ìˆ ì§€ì¹¨\n",
      "[OK] ì¤‘ì†Œê·œëª¨ ê±´ì„¤ì—…ì²´ ë³¸ì‚¬ì˜ ì•ˆì „ë³´ê±´ê´€ë¦¬ì— ê´€í•œ ì§€ì¹¨.md â†’ 08_general\n",
      "[âŒ ë¯¸ë¶„ë¥˜] ì¤‘ì†Œê·œëª¨ ê´€ë¡œê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md : ì¤‘ì†Œê·œëª¨ê´€ë¡œê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨\n",
      "[OK] ì§€í•˜ë§¤ì„¤ë¬¼ êµ´ì°©ê³µì‚¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] ì² ê³¨ê³µì‚¬ ë¬´ì§€ë³´ ê±°í‘¸ì§‘ë™ë°”ë¦¬(ë°í¬í”Œë ˆì´íŠ¸ ê³µë²•)ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[âŒ ë¯¸ë¶„ë¥˜] ì² ê³¨ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md : ì² ê³¨ê³µì‚¬ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨\n",
      "[âŒ ë¯¸ë¶„ë¥˜] ì² íƒ‘ê³µì‚¬ ì•ˆì „ë³´ê±´ê¸°ìˆ ì§€ì¹¨.md : ì² íƒ‘ê³µì‚¬ì•ˆì „ë³´ê±´ê¸°ìˆ ì§€ì¹¨\n",
      "[âŒ ë¯¸ë¶„ë¥˜] ì´ˆê³ ì¸µ ê±´ì¶•ë¬¼ê³µì‚¬(ì¼ë°˜ì‚¬í•­) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md : ì´ˆê³ ì¸µê±´ì¶•ë¬¼ê³µì‚¬(ì¼ë°˜ì‚¬í•­) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨\n",
      "[âŒ ë¯¸ë¶„ë¥˜] ì´ˆê³ ì¸µ ê±´ì¶•ë¬¼ê³µì‚¬(í™”ì¬ì˜ˆë°©) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md : ì´ˆê³ ì¸µê±´ì¶•ë¬¼ê³µì‚¬(í™”ì¬ì˜ˆë°©) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨\n",
      "[âŒ ë¯¸ë¶„ë¥˜] ì¶”ë½ë°©í˜¸ë§ ì„¤ì¹˜ ì§€ì¹¨.md : ì¶”ë½ë°©í˜¸ë§ì„¤ì¹˜ì§€ì¹¨\n",
      "[OK] ì·¨ì•½ì‹œê¸° ê±´ì„¤í˜„ì¥ ì•ˆì „ì‘ì—…ì§€ì¹¨.md â†’ 08_general\n",
      "[OK] ì½˜í¬ë¦¬íŠ¸ê³µì‚¬ì˜ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[OK] íƒ€ì›Œí¬ë ˆì¸ ì„¤ì¹˜, ì¡°ë¦½, í•´ì²´ ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] íƒ€ì¼(Tile) ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 06_finishing\n",
      "[OK] íƒ‘ë‹¤ìš´(Top down) ê³µë²• ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] í„°ë„ê³µì‚¬(NATMê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] í„°ë„ê³µì‚¬(NTRê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] í„°ë„ê³µì‚¬(Shield-T.B.Mê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] í„°ë„ê³µì‚¬(ì¹¨ë§¤ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] í„°ë„ê³µì‚¬(í”„ë¡ íŠ¸ì­í‚¹) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 03_tunnel\n",
      "[OK] íŠ¸ëŸ¬ìŠ¤ê±°ë” êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] íŠ¸ëŸ­ íƒ‘ì¬í˜• í¬ë ˆì¸(Cago crane) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] íŒŒì´í”„ ì„œí¬íŠ¸ ë™ë°”ë¦¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 04_scaffold\n",
      "[OK] í”„ë¦¬ìŠ¤íŠ¸ë ˆìŠ¤íŠ¸ ì½˜í¬ë¦¬íŠ¸(PSC) êµëŸ‰ê³µì‚¬ ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] í”„ë¦¬ìºìŠ¤íŠ¸ ì½˜í¬ë¦¬íŠ¸ ê±´ì¶•êµ¬ì¡°ë¬¼ ì¡°ë¦½ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 07_concrete\n",
      "[OK] í•­íƒ€ê¸°, í•­ë°œê¸° ì‚¬ìš© ì‘ì—…ê³„íšì„œ ì‘ì„±ì§€ì¹¨.md â†’ 05_crane\n",
      "[OK] í•´ìƒ RCD í˜„ì¥íƒ€ì„¤ ë§ëšê³µì‚¬(í˜„ìˆ˜êµ, ì‚¬ì¥êµ) ì•ˆì „ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] í•´ì²´ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ 08_general\n",
      "[OK] í˜„ìˆ˜êµ êµëŸ‰ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] í˜„ìˆ˜êµ ì£¼íƒ‘ì‹œê³µ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 01_bridge\n",
      "[OK] í™”í•™í”ŒëœíŠ¸ ê°œë³´ìˆ˜ ê³µì‚¬ ì•ˆì „ë³´ê±´ì‘ì—… ê¸°ìˆ ì§€ì¹¨.md â†’ 08_general\n",
      "[OK] í™ë§‰ì´ê³µì‚¬ (SCW ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(C.I.Pê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(Earth Anchor ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(Soil Nailing ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(ê°•ë„ë§ëš, Sheet Pile)ì˜ ì•ˆì „ë³´ê±´ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(ë ì¥ê¸´ì¥ê³µë²•, Prestressed Wale Method) ì•ˆì „ë³´ê±´ ì‘ì—…ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(ì—„ì§€ë§ëš ê³µë²•) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "[OK] í™ë§‰ì´ê³µì‚¬(ì§€í•˜ì—°ì†ë²½) ì•ˆì „ë³´ê±´ì‘ì—… ì§€ì¹¨.md â†’ 02_earth\n",
      "\n",
      "===== ğŸ“Š ë¶„ë¥˜ ìš”ì•½ =====\n",
      "01_bridge : 15ê°œ\n",
      "02_earth : 20ê°œ\n",
      "03_tunnel : 7ê°œ\n",
      "04_scaffold : 18ê°œ\n",
      "05_crane : 9ê°œ\n",
      "06_finishing : 9ê°œ\n",
      "07_concrete : 5ê°œ\n",
      "08_general : 8ê°œ\n",
      "\n",
      "ğŸ‰ DB ìë™ ë¶„ë¥˜ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# md íŒŒì¼ë“¤ì´ ë“¤ì–´ìˆëŠ” í´ë”\n",
    "md_dir = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/data_md\"\n",
    "\n",
    "# DB ìƒì„± ë£¨íŠ¸ í´ë”\n",
    "db_root = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/DB\"\n",
    "\n",
    "\n",
    "# ============ ìµœì¢… DB RULES ==============\n",
    "DB_RULES = {\n",
    "    \"01_bridge\": [\n",
    "        \"êµëŸ‰\", \"ê±°ë”\", \"ì•„ì¹˜êµ\", \"ì‚¬ì¥êµ\", \"í˜„ìˆ˜êµ\", \"PSM\", \"RCD\",\n",
    "        \"íŠ¸ëŸ¬ìŠ¤\", \"ë¼ë©˜\", \"MSS\"\n",
    "    ],\n",
    "    \"02_earth\": [\n",
    "        \"í™ë§‰ì´\", \"êµ´ì°©\", \"ê³„ì¸¡ê´€ë¦¬\", \"ì§€í•˜ë§¤ì„¤\", \"SCW\", \"CIP\", \n",
    "        \"Earth\", \"Nailing\", \"Sheet\", \"ì˜¹ë²½\", \"ë³´ê°•í† \",\n",
    "        \"ê¸°ì´ˆ\", \"ê´€ë¡œë§¤ì„¤\", \"ìš°ë¬¼í†µ\"\n",
    "    ],\n",
    "    \"03_tunnel\": [\n",
    "        \"í„°ë„\", \"NATM\", \"NTR\", \"TBM\", \"Shield\",\n",
    "        \"ì¹¨ë§¤\", \"ë°œíŒŒ\", \"í”„ë¡ íŠ¸ì­í‚¹\", \"Top down\", \"íƒ‘ë‹¤ìš´\"\n",
    "    ],\n",
    "    \"04_scaffold\": [\n",
    "        \"ë¹„ê³„\", \"ê°€ì„¤\", \"ì‘ì—…ë°œíŒ\", \"ë°©ì§€ë§\", \"ë°©í˜¸ì„ ë°˜\", \"ë³´í˜¸ë§\",\n",
    "        \"ë‹¬ë¹„ê³„\", \"ì¶”ë½ë°©ë§\", \"ê°€ì„¤ê³„ë‹¨\",\n",
    "        \"ê°±í¼\", \"Slip form\", \"ìŠ¬ë¦½í¼\", \"ë™ë°”ë¦¬\", \"í¼\", \"RCS\", \"ACS\",\n",
    "        \"ì„œí¬íŠ¸\", \"Gondola\", \"ê³¤ëŒë¼\"\n",
    "    ],\n",
    "    \"05_crane\": [\n",
    "        \"í¬ë ˆì¸\", \"ì–‘ì¤‘\", \"ê±´ì„¤ê¸°ê³„\", \"ì¤‘ëŸ‰ë¬¼\", \"íƒ€ì›Œí¬ë ˆì¸\",\n",
    "        \"í•­íƒ€ê¸°\", \"í•­ë°œê¸°\", \"ë¤í”„íŠ¸ëŸ­\", \"í™”ë¬¼ìë™ì°¨\",\n",
    "        \"Barge\", \"ë°”ì§€ì„ \", \"ê³ ì†Œì‘ì—…ëŒ€\"\n",
    "    ],\n",
    "    \"06_finishing\": [\n",
    "        \"ì„ê³µì‚¬\", \"ì»¤íŠ¼ì›”\", \"ë‚´ì¥\", \"ë¯¸ì¥\", \"íƒ€ì¼\", \n",
    "        \"ë‹¨ì—´\", \"ë°©ìˆ˜\", \"ì²œì¥\", \"ì¡°ê²½\", \"ì‹ì¬\"\n",
    "    ],\n",
    "    \"07_concrete\": [\n",
    "        \"ì½˜í¬ë¦¬íŠ¸\", \"ìŠ¬ë˜ë¸Œ\", \"íŒŒì¼í•­íƒ€\", \"í”„ë¦¬ìºìŠ¤íŠ¸\", \"PSC\", \"PC\"\n",
    "    ],\n",
    "    \"08_general\": [\n",
    "        \"ì•ˆì „ëŒ€\", \"ëŒê´€\", \"ì·¨ì•½ì‹œê¸°\", \"ì•¼ê°„\", \"ì„¤ê³„ ì§€ì¹¨\",\n",
    "        \"ë³¸ì‚¬\", \"ì•ˆì „ë³´ê±´ê´€ë¦¬\", \"ìš©ì ‘\", \"ìš©ë‹¨\",\n",
    "        \"í™”í•™í”ŒëœíŠ¸\", \"í•´ì²´ê³µì‚¬\"\n",
    "    ],\n",
    "}\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# DB í´ë” ìƒì„±\n",
    "os.makedirs(db_root, exist_ok=True)\n",
    "for db_name in DB_RULES.keys():\n",
    "    os.makedirs(os.path.join(db_root, db_name), exist_ok=True)\n",
    "\n",
    "# ë¶„ë¥˜ í•¨ìˆ˜\n",
    "def classify_md(title):\n",
    "    for db_name, keywords in DB_RULES.items():\n",
    "        for kw in keywords:\n",
    "            if kw in title:\n",
    "                return db_name\n",
    "    return None  # ì¼ë°˜ì ìœ¼ë¡œ ë‚˜ì˜¤ì§€ ì•Šì§€ë§Œ ì˜ˆì™¸ ëŒ€ë¹„\n",
    "\n",
    "# md íŒŒì¼ ìˆœíšŒ ë° ë¶„ë¥˜\n",
    "classified_count = {db: 0 for db in DB_RULES}\n",
    "\n",
    "for fname in sorted(os.listdir(md_dir)):\n",
    "    if fname.endswith(\".md\"):\n",
    "        src_path = os.path.join(md_dir, fname)\n",
    "\n",
    "        # md íŒŒì¼ ì œëª©(ì²« ë¼ì¸)\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            first_line = f.readline().strip()\n",
    "        \n",
    "        title = first_line.lstrip(\"# \").strip()\n",
    "        db_category = classify_md(title)\n",
    "\n",
    "        if db_category is None:\n",
    "            print(f\"[âŒ ë¯¸ë¶„ë¥˜] {fname} : {title}\")\n",
    "            continue\n",
    "\n",
    "        dst_path = os.path.join(db_root, db_category, fname)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "        classified_count[db_category] += 1\n",
    "        print(f\"[OK] {fname} â†’ {db_category}\")\n",
    "\n",
    "print(\"\\n===== ğŸ“Š ë¶„ë¥˜ ìš”ì•½ =====\")\n",
    "for db_name, count in classified_count.items():\n",
    "    print(f\"{db_name} : {count}ê°œ\")\n",
    "\n",
    "print(\"\\nğŸ‰ DB ìë™ ë¶„ë¥˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21cfa319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ğŸ“Œ MD ë¶„ë¥˜ ê²€ì¦ ê²°ê³¼ =====\n",
      "\n",
      "ğŸ“„ ì›ë³¸ md íŒŒì¼ ê°œìˆ˜: 99\n",
      "ğŸ“ DB í´ë”ì— ìˆëŠ” md íŒŒì¼ ê°œìˆ˜: 99\n",
      "\n",
      "â­• ëˆ„ë½ëœ íŒŒì¼ ì—†ìŒ\n",
      "\n",
      "â­• DBì—ë§Œ ì¡´ì¬í•˜ëŠ” ì˜ì‹¬ íŒŒì¼ ì—†ìŒ\n",
      "\n",
      "â­• ì¤‘ë³µ ë¶„ë¥˜ëœ íŒŒì¼ ì—†ìŒ\n",
      "\n",
      "===== ê²€ì‚¬ ì™„ë£Œ =====\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ì›ë³¸ md í´ë”\n",
    "md_dir = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/data_md\"\n",
    "\n",
    "# DBê°€ ìƒì„±ëœ ë£¨íŠ¸ í´ë”\n",
    "db_root = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/DB\"\n",
    "\n",
    "# Step 1. ì›ë³¸ md íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "original_files = sorted([f for f in os.listdir(md_dir) if f.endswith(\".md\")])\n",
    "\n",
    "# Step 2. DB í´ë”ì—ì„œ ìˆ˜ì§‘í•œ ëª¨ë“  md íŒŒì¼\n",
    "db_files = []\n",
    "db_map = {}  # íŒŒì¼ëª… â†’ DBí´ë”ëª… ë§¤í•‘\n",
    "\n",
    "for db_name in sorted(os.listdir(db_root)):\n",
    "    db_path = os.path.join(db_root, db_name)\n",
    "    if not os.path.isdir(db_path):\n",
    "        continue\n",
    "    \n",
    "    for f in os.listdir(db_path):\n",
    "        if f.endswith(\".md\"):\n",
    "            db_files.append(f)\n",
    "            db_map[f] = db_name\n",
    "\n",
    "# Step 3. set ë¹„êµ\n",
    "original_set = set(original_files)\n",
    "db_set = set(db_files)\n",
    "\n",
    "missing = original_set - db_set     # DBì— ëˆ„ë½ëœ íŒŒì¼\n",
    "extra = db_set - original_set       # ì›ë³¸ì—” ì—†ëŠ”ë° DBì—ë§Œ ìˆëŠ” íŒŒì¼\n",
    "duplicates = [f for f in db_files if db_files.count(f) > 1]\n",
    "\n",
    "\n",
    "# ============ ì¶œë ¥ ============\n",
    "\n",
    "print(\"\\n===== ğŸ“Œ MD ë¶„ë¥˜ ê²€ì¦ ê²°ê³¼ =====\\n\")\n",
    "\n",
    "print(f\"ğŸ“„ ì›ë³¸ md íŒŒì¼ ê°œìˆ˜: {len(original_files)}\")\n",
    "print(f\"ğŸ“ DB í´ë”ì— ìˆëŠ” md íŒŒì¼ ê°œìˆ˜: {len(db_files)}\\n\")\n",
    "\n",
    "\n",
    "# 1) ëˆ„ë½ëœ íŒŒì¼\n",
    "if missing:\n",
    "    print(\"âŒ DBì— ëˆ„ë½ëœ íŒŒì¼:\")\n",
    "    for m in sorted(missing):\n",
    "        print(f\"   - {m}\")\n",
    "else:\n",
    "    print(\"â­• ëˆ„ë½ëœ íŒŒì¼ ì—†ìŒ\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# 2) DBì—ë§Œ ì¡´ì¬í•˜ëŠ” ì´ìƒ íŒŒì¼\n",
    "if extra:\n",
    "    print(\"âš ï¸ ì›ë³¸ì—” ì—†ê³  DBì—ë§Œ ì¡´ì¬í•˜ëŠ” íŒŒì¼:\")\n",
    "    for e in sorted(extra):\n",
    "        print(f\"   - {e}\")\n",
    "else:\n",
    "    print(\"â­• DBì—ë§Œ ì¡´ì¬í•˜ëŠ” ì˜ì‹¬ íŒŒì¼ ì—†ìŒ\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "# 3) ì¤‘ë³µ ë¶„ë¥˜ëœ íŒŒì¼\n",
    "if duplicates:\n",
    "    print(\"âš ï¸ ì¤‘ë³µìœ¼ë¡œ ì—¬ëŸ¬ DBì— í¬í•¨ëœ íŒŒì¼:\")\n",
    "    seen = set()\n",
    "    for d in duplicates:\n",
    "        if d not in seen:\n",
    "            print(f\"   - {d}  (ì˜ˆ: {db_map.get(d)})\")\n",
    "            seen.add(d)\n",
    "else:\n",
    "    print(\"â­• ì¤‘ë³µ ë¶„ë¥˜ëœ íŒŒì¼ ì—†ìŒ\")\n",
    "\n",
    "print(\"\\n===== ê²€ì‚¬ ì™„ë£Œ =====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b87d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 01_bridge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01_bridge ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 2654.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ â†’ /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md/01_bridge/01_bridge_chunks.jsonl (ì´ 436 ì²­í¬)\n",
      "\n",
      "ğŸ“ Processing DB: 02_earth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02_earth ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 3281.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ â†’ /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md/02_earth/02_earth_chunks.jsonl (ì´ 557 ì²­í¬)\n",
      "\n",
      "ğŸ“ Processing DB: 03_tunnel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03_tunnel ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 2531.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ â†’ /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md/03_tunnel/03_tunnel_chunks.jsonl (ì´ 244 ì²­í¬)\n",
      "\n",
      "ğŸ“ Processing DB: 04_scaffold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04_scaffold ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 4210.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ â†’ /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md/04_scaffold/04_scaffold_chunks.jsonl (ì´ 360 ì²­í¬)\n",
      "\n",
      "ğŸ“ Processing DB: 05_crane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05_crane ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 2236.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ â†’ /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md/05_crane/05_crane_chunks.jsonl (ì´ 332 ì²­í¬)\n",
      "\n",
      "ğŸ“ Processing DB: 06_finishing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06_finishing ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 3884.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ â†’ /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md/06_finishing/06_finishing_chunks.jsonl (ì´ 207 ì²­í¬)\n",
      "\n",
      "ğŸ“ Processing DB: 07_concrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07_concrete ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 3117.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ â†’ /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md/07_concrete/07_concrete_chunks.jsonl (ì´ 175 ì²­í¬)\n",
      "\n",
      "ğŸ“ Processing DB: 08_general\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08_general ì²˜ë¦¬ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 2250.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ â†’ /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md/08_general/08_general_chunks.jsonl (ì´ 359 ì²­í¬)\n",
      "\n",
      "ğŸ‰ ëª¨ë“  DB ì²­í‚¹ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ===== 1ï¸âƒ£ ê²½ë¡œ ì„¤ì • =====\n",
    "DB_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md\"\n",
    "\n",
    "# ===== 2ï¸âƒ£ ì„¹ì…˜ ë¶„ë¦¬ í•¨ìˆ˜ =====\n",
    "def split_by_heading(text: str):\n",
    "    \"\"\"# í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ”\"\"\"\n",
    "    sections = re.split(r'(?=^# )', text, flags=re.MULTILINE)\n",
    "    return [s.strip() for s in sections if s.strip()]\n",
    "\n",
    "# ===== 3ï¸âƒ£ ë‚´ë¶€ ì²­í‚¹ Splitter =====\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "# ===== 4ï¸âƒ£ ì „ì²´ DB í´ë” ìˆœíšŒ =====\n",
    "for db_folder in sorted(os.listdir(DB_ROOT)):\n",
    "    db_path = os.path.join(DB_ROOT, db_folder)\n",
    "    if not os.path.isdir(db_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“ Processing DB: {db_folder}\")\n",
    "\n",
    "    md_files = [f for f in os.listdir(db_path) if f.endswith(\".md\")]\n",
    "    chunks = []\n",
    "\n",
    "    # ===== 5ï¸âƒ£ md íŒŒì¼ ìˆœíšŒ =====\n",
    "    for file_name in tqdm(md_files, desc=f\"{db_folder} ì²˜ë¦¬ ì¤‘\"):\n",
    "        file_path = os.path.join(db_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_name} ({e})\")\n",
    "            continue\n",
    "\n",
    "        # ì„¹ì…˜ ë¶„ë¦¬\n",
    "        sections = split_by_heading(raw_text)\n",
    "\n",
    "        # ê° ì„¹ì…˜ ë‚´ë¶€ ì²­í‚¹\n",
    "        for sec in sections:\n",
    "            heading_match = re.match(r\"^#\\s*(.*)\", sec)\n",
    "            heading = heading_match.group(1).strip() if heading_match else \"ë³¸ë¬¸\"\n",
    "\n",
    "            split_texts = splitter.split_text(sec)\n",
    "            for idx, chunk in enumerate(split_texts):\n",
    "                chunks.append({\n",
    "                    \"db\": db_folder,                     # ì–´ë–¤ DBì—ì„œ ë‚˜ì˜¨ ì²­í¬ì¸ê°€?\n",
    "                    \"file\": file_name,                   # ì›ë³¸ ë¬¸ì„œëª…\n",
    "                    \"section\": heading,                  # í—¤ë” ì œëª©\n",
    "                    \"chunk_id\": idx,                     # ë¬¸ì„œ ë‚´ ì²­í¬ ë²ˆí˜¸\n",
    "                    \"source_path\": file_path,            # ì „ì²´ ê²½ë¡œ\n",
    "                    \"content\": chunk                     # ì‹¤ì œ í…ìŠ¤íŠ¸\n",
    "                })\n",
    "\n",
    "    # ===== 6ï¸âƒ£ DBë³„ chunks.jsonl ì €ì¥ =====\n",
    "    out_path = os.path.join(db_path, f\"{db_folder}_chunks.jsonl\")\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "        for c in chunks:\n",
    "            jf.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ â†’ {out_path} (ì´ {len(chunks)} ì²­í¬)\")\n",
    "\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  DB ì²­í‚¹ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ae4107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“ DB í´ë” ë³µì œ ì¤‘: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 314.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… DB â†’ DB2 ë³€í™˜ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "DB_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/DB\"\n",
    "DB2_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/DB2\"\n",
    "\n",
    "# DB2 ë£¨íŠ¸ í´ë” ìƒì„±\n",
    "os.makedirs(DB2_ROOT, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_hash_depth(line):\n",
    "    \"\"\"'#', '##', '###' ì˜ ê¹Šì´ ê³„ì‚°\"\"\"\n",
    "    match = re.match(r\"^(#+)\\s*\", line)\n",
    "    return len(match.group(1)) if match else 0\n",
    "\n",
    "\n",
    "def extract_numbering_and_title(line):\n",
    "    \"\"\"\n",
    "    '# 5.2 ì„¤ì¹˜ì‘ì—…' â†’ ('5.2', 'ì„¤ì¹˜ì‘ì—…')\n",
    "    \"\"\"\n",
    "    line_no_hash = re.sub(r\"^#+\\s*\", \"\", line).strip()\n",
    "    match = re.match(r\"^(\\d+(?:\\.\\d+)*)(?:\\s*)(.*)\", line_no_hash)\n",
    "    if not match:\n",
    "        return None, None\n",
    "    return match.group(1), match.group(2).strip()\n",
    "\n",
    "\n",
    "def numbering_to_depth(numbering):\n",
    "    \"\"\"'5.2.1' â†’ 3\"\"\"\n",
    "    return numbering.count(\".\") + 1\n",
    "\n",
    "\n",
    "def process_md_line(line):\n",
    "    stripped = line.strip()\n",
    "\n",
    "    # --- 1) '#' ì—†ëŠ” ë¼ì¸ì€ ê·¸ëŒ€ë¡œ ---\n",
    "    if not stripped.startswith(\"#\"):\n",
    "        return line\n",
    "\n",
    "    # --- 2) numbering ì¶”ì¶œ ---\n",
    "    numbering, title = extract_numbering_and_title(stripped)\n",
    "    if not numbering:\n",
    "        return line   # ìˆ«ìë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ì œëª©ì´ ì•„ë‹˜ â†’ ê·¸ëŒ€ë¡œ ë‘ \n",
    "\n",
    "    # --- 3) í˜„ì¬ '#' ê¹Šì´ ---\n",
    "    current_depth = get_hash_depth(stripped)\n",
    "\n",
    "    # --- 4) ì˜¬ë°”ë¥¸ '#' ê¹Šì´ ê³„ì‚° ---\n",
    "    correct_depth = numbering_to_depth(numbering)\n",
    "\n",
    "    # --- 5) ì´ë¯¸ ì˜¬ë°”ë¥´ë©´ ìˆ˜ì •í•˜ì§€ ì•ŠìŒ ---\n",
    "    if current_depth == correct_depth:\n",
    "        return line\n",
    "\n",
    "    # --- 6) ìˆ˜ì •í•´ì•¼ í•˜ëŠ” ê²½ìš° ----\n",
    "    hashes = \"#\" * correct_depth\n",
    "    new_line = f\"{hashes} {numbering}\"\n",
    "    if title:\n",
    "        new_line += f\" {title}\"\n",
    "\n",
    "    return new_line + \"\\n\"\n",
    "\n",
    "\n",
    "def process_md_file(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    new_lines = [process_md_line(line) for line in lines]\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(new_lines)\n",
    "\n",
    "\n",
    "# ============================\n",
    "#     DB â†’ DB2 ì „ì²´ ë³µì œ ì‹¤í–‰\n",
    "# ============================\n",
    "\n",
    "for folder in tqdm(os.listdir(DB_ROOT), desc=\"ğŸ“ DB í´ë” ë³µì œ ì¤‘\"):\n",
    "    folder_path = os.path.join(DB_ROOT, folder)\n",
    "\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    # DB2ì—ì„œ ë™ì¼ í´ë” ìƒì„±\n",
    "    new_folder_path = os.path.join(DB2_ROOT, folder)\n",
    "    os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "    for fname in os.listdir(folder_path):\n",
    "        old_file = os.path.join(folder_path, fname)\n",
    "        new_file = os.path.join(new_folder_path, fname)\n",
    "\n",
    "        # md íŒŒì¼ì´ë©´ ì²˜ë¦¬\n",
    "        if fname.endswith(\".md\"):\n",
    "            process_md_file(old_file, new_file)\n",
    "        else:\n",
    "            # mdê°€ ì•„ë‹Œ íŒŒì¼ì€ ê·¸ëŒ€ë¡œ ë³µì‚¬\n",
    "            shutil.copy(old_file, new_file)\n",
    "\n",
    "print(\"\\nâœ… DB â†’ DB2 ë³€í™˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00632bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 01_bridge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01_bridge: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 1389.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 02_earth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02_earth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 1526.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 03_tunnel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03_tunnel: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 968.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 04_scaffold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04_scaffold: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 1426.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 05_crane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05_crane: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 898.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 06_finishing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06_finishing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1741.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 07_concrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07_concrete: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 1510.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 08_general\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08_general: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 943.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ ëª¨ë“  DB2 â†’ DB2_chunks ì²­í‚¹ + Hierarchy ìƒì„± ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "DB2_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md\"\n",
    "OUTPUT_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/DB_chunks\"\n",
    "\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) ë¬¸ì„œ ì „ì²´ì—ì„œ Headingì„ ì¶”ì¶œí•´ ì „ì²´ ê³„ì¸µ íŠ¸ë¦¬ êµ¬ì„±\n",
    "# ------------------------------------------------------------\n",
    "def extract_all_headings(text):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ì „ì²´ ì œëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜\n",
    "    ex: [{'number': '5', 'title':'ì£¼íƒ‘ì•ˆì „ì‘ì—…','depth':1}, ...]\n",
    "    \"\"\"\n",
    "    headings = []\n",
    "    pattern = r\"^(#{1,4})\\s+(\\d+(?:\\.\\d+)*)(?:\\s*)(.*)$\"\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        m = re.match(pattern, line.strip())\n",
    "        if m:\n",
    "            hashes, number, title = m.groups()\n",
    "            depth = len(hashes)\n",
    "            headings.append({\n",
    "                \"number\": number,\n",
    "                \"title\": title.strip(),\n",
    "                \"depth\": depth\n",
    "            })\n",
    "\n",
    "    return headings\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) íŠ¹ì • section_numberë¥¼ ê¸°ì¤€ìœ¼ë¡œ hierarchy ìë™ êµ¬ì„±\n",
    "# ------------------------------------------------------------\n",
    "def build_hierarchy(all_headings, target_number):\n",
    "    \"\"\"\n",
    "    ex: target_number=\"5.2.1\" â†’ ìƒìœ„ \"5\", \"5.2\" ìë™ í¬í•¨\n",
    "    \"\"\"\n",
    "    target_levels = target_number.split(\".\")\n",
    "    hierarchy = []\n",
    "\n",
    "    for h in all_headings:\n",
    "        h_levels = h[\"number\"].split(\".\")\n",
    "\n",
    "        if len(h_levels) <= len(target_levels):\n",
    "            if h_levels == target_levels[:len(h_levels)]:\n",
    "                hierarchy.append(f\"{h['number']} {h['title']}\")\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) Heading ë‹¨ìœ„ë¡œ ì„¹ì…˜ ë¶„ë¦¬\n",
    "# ------------------------------------------------------------\n",
    "def split_by_sections(text):\n",
    "    pattern = r\"(?=^#{1,4}\\s+\\d+(?:\\.\\d+)* )\"\n",
    "    sections = re.split(pattern, text, flags=re.MULTILINE)\n",
    "\n",
    "    cleaned = []\n",
    "    for sec in sections:\n",
    "        sec = sec.strip()\n",
    "        if sec.startswith(\"#\"):\n",
    "            cleaned.append(sec)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4) ê°œë³„ section í—¤ë” ì •ë³´ íŒŒì‹±\n",
    "# ------------------------------------------------------------\n",
    "def extract_section_heading(section_text):\n",
    "    first_line = section_text.split(\"\\n\")[0].strip()\n",
    "    m = re.match(r\"^(#{1,4})\\s+(\\d+(?:\\.\\d+)*)(?:\\s*)(.*)$\", first_line)\n",
    "\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "\n",
    "    hashes, number, title = m.groups()\n",
    "    heading_text = f\"{number} {title}\".strip()\n",
    "    depth = len(hashes)\n",
    "\n",
    "    return heading_text, number, depth\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5) Chunk splitter ì„¤ì •\n",
    "# ------------------------------------------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6) DB2 ì „ì²´ ì²­í‚¹ ìˆ˜í–‰\n",
    "# ------------------------------------------------------------\n",
    "for db_name in sorted(os.listdir(DB2_ROOT)):\n",
    "    db_path = os.path.join(DB2_ROOT, db_name)\n",
    "    if not os.path.isdir(db_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“ Processing DB: {db_name}\")\n",
    "\n",
    "    # â”€ DB ì´ë¦„ ê·¸ëŒ€ë¡œ í´ë” ìƒì„± â”€\n",
    "    out_folder = os.path.join(OUTPUT_ROOT, db_name)\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "    out_jsonl = os.path.join(out_folder, f\"{db_name}_chunks.jsonl\")\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out_f:\n",
    "\n",
    "        # md íŒŒì¼ë“¤ íƒìƒ‰\n",
    "        for file_name in tqdm(os.listdir(db_path), desc=db_name):\n",
    "            if not file_name.endswith(\".md\"):\n",
    "                continue\n",
    "\n",
    "            md_path = os.path.join(db_path, file_name)\n",
    "\n",
    "            # â”€ Raw text ì½ê¸° â”€\n",
    "            with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_text = f.read()\n",
    "\n",
    "            # â”€ ë¬¸ì„œ ì „ì²´ Heading ìˆ˜ì§‘ â”€\n",
    "            all_headings = extract_all_headings(raw_text)\n",
    "\n",
    "            # â”€ ì„¹ì…˜ ë¶„ë¦¬ â”€\n",
    "            sections = split_by_sections(raw_text)\n",
    "\n",
    "            for sec in sections:\n",
    "                section_text, section_number, depth = extract_section_heading(sec)\n",
    "                if not section_number:\n",
    "                    continue\n",
    "\n",
    "                # â”€ Hierarchy ìë™ êµ¬ì„± â”€\n",
    "                hierarchy_list = build_hierarchy(all_headings, section_number)\n",
    "                hierarchy_str = \" > \".join(hierarchy_list)\n",
    "\n",
    "                # ì„¹ì…˜ ë³¸ë¬¸ ì¶”ì¶œ\n",
    "                body = sec.split(\"\\n\", 1)[1] if \"\\n\" in sec else \"\"\n",
    "\n",
    "                # â”€ ë‚´ë¶€ chunking â”€\n",
    "                chunks = splitter.split_text(body)\n",
    "\n",
    "                # â”€ Chunk ì €ì¥ â”€\n",
    "                for idx, chunk in enumerate(chunks):\n",
    "\n",
    "                    record = {\n",
    "                        \"db\": db_name,\n",
    "                        \"file\": file_name,\n",
    "                        \"section\": section_text,\n",
    "                        \"section_number\": section_number,\n",
    "                        \"hierarchy\": hierarchy_list,\n",
    "                        \"hierarchy_str\": hierarchy_str,\n",
    "                        \"chunk_id\": idx,\n",
    "                        \"source_path\": md_path,\n",
    "                        \"content\": chunk\n",
    "                    }\n",
    "\n",
    "                    out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  DB2 â†’ DB2_chunks ì²­í‚¹ + Hierarchy ìƒì„± ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f87ea16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ Chunk Quality Report ìƒì„± ì™„ë£Œ!\n",
      "ğŸ“„ ì €ì¥ ìœ„ì¹˜: /home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/chunk_quality_report.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "CHUNK_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/DB_chunks_cleaned\"\n",
    "REPORT_PATH = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/chunk_quality_report.json\"\n",
    "\n",
    "# ìœ ì‚¬ë„ ì¤‘ë³µ ê²€ì‚¬ ëª¨ë¸ (ë¹ ë¥´ê³  ê°€ë³ê³  ì¢‹ìŒ)\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def section_depth(num):\n",
    "    return len(num.split(\".\"))\n",
    "\n",
    "\n",
    "report = {}\n",
    "\n",
    "# ================================\n",
    "# DBë³„ í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘\n",
    "# ================================\n",
    "for db_name in sorted(os.listdir(CHUNK_ROOT)):\n",
    "    db_folder = os.path.join(CHUNK_ROOT, db_name)\n",
    "    if not os.path.isdir(db_folder):\n",
    "        continue\n",
    "\n",
    "    jsonl_path = os.path.join(db_folder, f\"{db_name}_chunks.jsonl\")\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ” Checking DB: {db_name}\")\n",
    "    chunks = load_jsonl(jsonl_path)\n",
    "\n",
    "    total_chunks = len(chunks)\n",
    "\n",
    "    too_short = 0\n",
    "    too_long = 0\n",
    "    hierarchy_missing = 0\n",
    "    hierarchy_mismatch = 0\n",
    "    problematic_sections = []\n",
    "\n",
    "    # ê¸¸ì´ ê²€ì‚¬\n",
    "    for c in chunks:\n",
    "        length = len(c[\"content\"])\n",
    "        if length < 50:\n",
    "            too_short += 1\n",
    "            problematic_sections.append({\"section\": c[\"section\"], \"reason\": \"too short\"})\n",
    "        if length > 2000:\n",
    "            too_long += 1\n",
    "            problematic_sections.append({\"section\": c[\"section\"], \"reason\": \"too long\"})\n",
    "\n",
    "        # hierarchy ëˆ„ë½ ê²€ì‚¬\n",
    "        if not c[\"hierarchy\"]:\n",
    "            hierarchy_missing += 1\n",
    "            problematic_sections.append({\"section\": c[\"section\"], \"reason\": \"hierarchy missing\"})\n",
    "\n",
    "        # hierarchy depth mismatch ê²€ì‚¬\n",
    "        if len(c[\"hierarchy\"]) != section_depth(c[\"section_number\"]):\n",
    "            hierarchy_mismatch += 1\n",
    "            problematic_sections.append({\"section\": c[\"section\"], \"reason\": \"hierarchy mismatch\"})\n",
    "\n",
    "\n",
    "    # ì¤‘ë³µ ê²€ì‚¬ (ë¬¸ì¥ì´ ê¸´ chunkë§Œ ì„ íƒí•˜ì—¬ ê²€ì‚¬ ë¶€ë‹´ ì¤„ì´ê¸°)\n",
    "    print(\"   ğŸ‘‰ Checking duplicate similarity (this may take ~15s)\")\n",
    "    texts = [c[\"content\"] for c in chunks]\n",
    "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "    cosine = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    duplicate_count = 0\n",
    "    for i in range(total_chunks):\n",
    "        for j in range(i + 1, total_chunks):\n",
    "            if cosine[i][j] > 0.95:\n",
    "                duplicate_count += 1\n",
    "                problematic_sections.append({\n",
    "                    \"section\": chunks[i][\"section\"],\n",
    "                    \"reason\": \"duplicate chunk (>0.95 cosine)\"\n",
    "                })\n",
    "\n",
    "    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n",
    "    error_rate = (\n",
    "        too_short +\n",
    "        too_long +\n",
    "        hierarchy_missing +\n",
    "        hierarchy_mismatch +\n",
    "        duplicate_count\n",
    "    ) / total_chunks\n",
    "\n",
    "    quality_score = round(max(0, 1 - error_rate), 3)\n",
    "\n",
    "\n",
    "    # DBë³„ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    report[db_name] = {\n",
    "        \"total_chunks\": total_chunks,\n",
    "        \"too_short\": too_short,\n",
    "        \"too_long\": too_long,\n",
    "        \"hierarchy_missing\": hierarchy_missing,\n",
    "        \"hierarchy_mismatch\": hierarchy_mismatch,\n",
    "        \"duplicate_chunks\": duplicate_count,\n",
    "        \"problematic_sections\": problematic_sections[:50],  # ë„ˆë¬´ ë§ìœ¼ë©´ 50ê°œë§Œ\n",
    "        \"quality_score\": quality_score\n",
    "    }\n",
    "\n",
    "\n",
    "# ================================\n",
    "# ìµœì¢… ë¦¬í¬íŠ¸ JSON ì €ì¥\n",
    "# ================================\n",
    "with open(REPORT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nğŸ‰ Chunk Quality Report ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“„ ì €ì¥ ìœ„ì¹˜: {REPORT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f38052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Evaluating CLEAN DB: 01_bridge\n",
      "\n",
      "ğŸ” Evaluating CLEAN DB: 02_earth\n",
      "\n",
      "ğŸ” Evaluating CLEAN DB: 03_tunnel\n",
      "\n",
      "ğŸ” Evaluating CLEAN DB: 04_scaffold\n",
      "\n",
      "ğŸ” Evaluating CLEAN DB: 05_crane\n",
      "\n",
      "ğŸ” Evaluating CLEAN DB: 06_finishing\n",
      "\n",
      "ğŸ” Evaluating CLEAN DB: 07_concrete\n",
      "\n",
      "ğŸ” Evaluating CLEAN DB: 08_general\n",
      "\n",
      "ğŸ“„ JSON Report saved to:\n",
      "/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/db_quality_report_clean.json\n",
      "\n",
      "\n",
      "================== CLEAN CHUNK QUALITY SUMMARY ==================\n",
      "\n",
      "DB Name         Total    Mismatch   Dup      Score \n",
      "------------------------------------------------------------\n",
      "01_bridge       168      0          0        1.0   \n",
      "02_earth        253      0          0        1.0   \n",
      "03_tunnel       129      0          0        1.0   \n",
      "04_scaffold     161      0          0        1.0   \n",
      "05_crane        180      0          0        1.0   \n",
      "06_finishing    85       0          0        1.0   \n",
      "07_concrete     63       0          0        1.0   \n",
      "08_general      181      0          0        1.0   \n",
      "\n",
      "================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# ============================\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "# ============================\n",
    "ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data\"\n",
    "CLEAN_DIR = f\"{ROOT}/DB_chunks_cleaned\"\n",
    "OUT_REPORT = f\"{ROOT}/db_quality_report_clean.json\"\n",
    "\n",
    "# ============================\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "# ============================\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ============================\n",
    "# 1) Hierarchy mismatch ì²´í¬ í•¨ìˆ˜\n",
    "# ============================\n",
    "def detect_hierarchy_mismatch(section):\n",
    "    pattern = r\"^(#+)\\s+(\\d+(\\.\\d+){0,2})\"\n",
    "    m = re.match(pattern, section)\n",
    "    if not m:\n",
    "        return True  # ìˆ«ì í—¤ë” êµ¬ì¡°ê°€ ì•„ë‹ˆë©´ mismatch\n",
    "\n",
    "    hashes = m.group(1)\n",
    "    numbering = m.group(2)\n",
    "\n",
    "    depth_expected = numbering.count(\".\") + 1\n",
    "    depth_actual = len(hashes)\n",
    "\n",
    "    return depth_expected != depth_actual\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2) Duplicate chunk íƒì§€\n",
    "# ============================\n",
    "def detect_duplicates(chunks, threshold=0.95):\n",
    "    docs = [c[\"content\"] for c in chunks]\n",
    "    embeds = model.encode(docs, convert_to_tensor=True)\n",
    "\n",
    "    duplicates = []\n",
    "    used = set()\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        if i in used:\n",
    "            continue\n",
    "\n",
    "        sims = util.cos_sim(embeds[i], embeds)[0].cpu().numpy()\n",
    "        dup_idx = np.where(sims >= threshold)[0]\n",
    "\n",
    "        for j in dup_idx:\n",
    "            if j != i:\n",
    "                duplicates.append((i, j, chunks[j][\"section\"]))\n",
    "                used.add(j)\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ğŸ”¥ CLEAN DB ì „ì²´ í‰ê°€\n",
    "# ============================\n",
    "results = {}\n",
    "\n",
    "for db in sorted(os.listdir(CLEAN_DIR)):\n",
    "    db_path = os.path.join(CLEAN_DIR, db)\n",
    "    if not os.path.isdir(db_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ” Evaluating CLEAN DB: {db}\")\n",
    "\n",
    "    jsonl_path = os.path.join(db_path, f\"{db}_chunks_cleaned.jsonl\")\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        print(\"âš  íŒŒì¼ ì—†ìŒ â€” SKIP\")\n",
    "        continue\n",
    "\n",
    "    chunks = []\n",
    "    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            chunks.append(json.loads(line))\n",
    "\n",
    "    total = len(chunks)\n",
    "\n",
    "    # === Hierarchy mismatch ===\n",
    "    hierarchy_mismatch = []\n",
    "    for c in chunks:\n",
    "        if detect_hierarchy_mismatch(c[\"section\"]):\n",
    "            hierarchy_mismatch.append(c[\"section\"])\n",
    "\n",
    "    # === Duplicate detection ===\n",
    "    duplicate_chunks = detect_duplicates(chunks)\n",
    "\n",
    "    # === Score ê³„ì‚° ===\n",
    "    score = (\n",
    "        1\n",
    "        - (len(hierarchy_mismatch) / total) * 0.5\n",
    "        - (len(duplicate_chunks) / total) * 0.5\n",
    "    )\n",
    "\n",
    "    results[db] = {\n",
    "        \"total_chunks\": total,\n",
    "        \"hierarchy_mismatch\": len(hierarchy_mismatch),\n",
    "        \"duplicate_chunks\": len(duplicate_chunks),\n",
    "        \"problematic_sections\": list(set(hierarchy_mismatch)),\n",
    "        \"quality_score\": round(score, 3),\n",
    "    }\n",
    "\n",
    "# ============================\n",
    "# JSON ë¦¬í¬íŠ¸ ì €ì¥\n",
    "# ============================\n",
    "with open(OUT_REPORT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ“„ JSON Report saved to:\\n{OUT_REPORT}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# ğŸ“Œ ë³´ê¸° ì¢‹ì€ í‘œ í˜•íƒœì˜ REPORT ì¶œë ¥\n",
    "# ============================\n",
    "print(\"\\n\\n================== CLEAN CHUNK QUALITY SUMMARY ==================\\n\")\n",
    "print(f\"{'DB Name':<15} {'Total':<8} {'Mismatch':<10} {'Dup':<8} {'Score':<6}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for db, r in results.items():\n",
    "    print(\n",
    "        f\"{db:<15} \"\n",
    "        f\"{r['total_chunks']:<8} \"\n",
    "        f\"{r['hierarchy_mismatch']:<10} \"\n",
    "        f\"{r['duplicate_chunks']:<8} \"\n",
    "        f\"{r['quality_score']:<6}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n================================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c78acec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 01_bridge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 1020.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 02_earth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:00<00:00, 1193.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 03_tunnel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 782.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 04_scaffold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:00<00:00, 1534.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 05_crane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 850.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 06_finishing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1450.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 07_concrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 1146.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 08_general\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 509.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ ëª¨ë“  FinalDB_md â†’ FinalDB_chunks ë³€í™˜ ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Root paths\n",
    "DB_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md\"\n",
    "OUT_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_chunks\"\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 1) Header ì¶”ì¶œ ( #, ##, ### )\n",
    "# ----------------------------------------------\n",
    "HEADER_PATTERN = r\"^(#{1,3})\\s*(\\d+(?:\\.\\d+)*)?\\s*(.*)$\"\n",
    "\n",
    "\n",
    "def extract_headers(text):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ì „ì²´ì—ì„œ #, ##, ### í˜•íƒœ í—¤ë” ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    headers = []\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        m = re.match(HEADER_PATTERN, line.strip())\n",
    "        if m:\n",
    "            hashes, number, title = m.groups()\n",
    "            depth = len(hashes)\n",
    "\n",
    "            headers.append({\n",
    "                \"depth\": depth,                 # 1 / 2 / 3\n",
    "                \"number\": number.strip() if number else None,\n",
    "                \"title\": title.strip(),\n",
    "                \"text\": line.strip()\n",
    "            })\n",
    "    return headers\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2) Section ë‹¨ìœ„ Split\n",
    "# ----------------------------------------------\n",
    "def split_sections(text):\n",
    "    \"\"\"\n",
    "    #, ##, ### ì‹œì‘ì ì„ ê¸°ì¤€ìœ¼ë¡œ section ë¶„ë¦¬\n",
    "    \"\"\"\n",
    "    pattern = r\"(?=^#{1,3}\\s*)\"\n",
    "    sections = re.split(pattern, text, flags=re.MULTILINE)\n",
    "\n",
    "    cleaned = [s.strip() for s in sections if s.strip().startswith(\"#\")]\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 3) ê°œë³„ Section í—¤ë” ë¶„ì„\n",
    "# ----------------------------------------------\n",
    "def parse_section_header(sec_text):\n",
    "    first_line = sec_text.split(\"\\n\")[0].strip()\n",
    "    m = re.match(HEADER_PATTERN, first_line)\n",
    "\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "\n",
    "    hashes, number, title = m.groups()\n",
    "    depth = len(hashes)\n",
    "    heading = f\"{number} {title}\".strip() if number else title\n",
    "\n",
    "    return heading, number, depth\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 4) Hierarchy êµ¬ì„±\n",
    "# ----------------------------------------------\n",
    "def build_hierarchy(all_headers, current_number, current_depth):\n",
    "    \"\"\"\n",
    "    current_number = \"4.2.1\"\n",
    "    â†’ depth=3 ì´ë©´ ìƒìœ„ \"4\", \"4.2\" í¬í•¨\n",
    "    \"\"\"\n",
    "    if not current_number:\n",
    "        return []\n",
    "\n",
    "    parts = current_number.split(\".\")\n",
    "    hierarchy = []\n",
    "\n",
    "    for h in all_headers:\n",
    "        if not h[\"number\"]:\n",
    "            continue\n",
    "\n",
    "        h_parts = h[\"number\"].split(\".\")\n",
    "\n",
    "        # ìƒìœ„ êµ¬ì¡° í¬í•¨\n",
    "        if len(h_parts) <= len(parts):\n",
    "            if h_parts == parts[:len(h_parts)]:\n",
    "                hierarchy.append(f\"{h['number']} {h['title']}\")\n",
    "\n",
    "    return hierarchy\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5) Chunk Splitter\n",
    "# ----------------------------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 6) ë©”ì¸ ì²˜ë¦¬\n",
    "# ----------------------------------------------\n",
    "for db_name in sorted(os.listdir(DB_ROOT)):\n",
    "    db_path = os.path.join(DB_ROOT, db_name)\n",
    "    if not os.path.isdir(db_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“ Processing DB: {db_name}\")\n",
    "\n",
    "    out_folder = os.path.join(OUT_ROOT, db_name)\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "    out_jsonl = os.path.join(out_folder, f\"{db_name}_chunks.jsonl\")\n",
    "\n",
    "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as out_f:\n",
    "\n",
    "        for file_name in tqdm(os.listdir(db_path)):\n",
    "            if not file_name.endswith(\".md\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(db_path, file_name)\n",
    "\n",
    "            # ì „ì²´ ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¡œë“œ\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_text = f.read()\n",
    "\n",
    "            # ë¬¸ì„œ ì „ì²´ í—¤ë” ëª©ë¡\n",
    "            all_headers = extract_headers(raw_text)\n",
    "\n",
    "            # Section ë¶„ë¦¬\n",
    "            sections = split_sections(raw_text)\n",
    "\n",
    "            # ê° Section ì²˜ë¦¬\n",
    "            for sec in sections:\n",
    "                section_heading, section_number, depth = parse_section_header(sec)\n",
    "                if not section_heading:\n",
    "                    continue\n",
    "\n",
    "                hierarchy_list = build_hierarchy(all_headers, section_number, depth)\n",
    "                hierarchy_str = \" > \".join(hierarchy_list)\n",
    "\n",
    "                # ë³¸ë¬¸ ì¶”ì¶œ\n",
    "                body = sec.split(\"\\n\", 1)[1] if \"\\n\" in sec else \"\"\n",
    "\n",
    "                # Chunk ìƒì„±\n",
    "                chunks = splitter.split_text(body)\n",
    "\n",
    "                for idx, chunk in enumerate(chunks):\n",
    "\n",
    "                    record = {\n",
    "                        \"db\": db_name,\n",
    "                        \"file\": file_name,\n",
    "                        \"section\": section_heading,\n",
    "                        \"section_number\": section_number,\n",
    "                        \"hierarchy\": hierarchy_list,\n",
    "                        \"hierarchy_str\": hierarchy_str,\n",
    "                        \"chunk_id\": idx,\n",
    "                        \"source_path\": file_path,\n",
    "                        \"content\": chunk.strip()\n",
    "                    }\n",
    "\n",
    "                    out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  FinalDB_md â†’ FinalDB_chunks ë³€í™˜ ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84fb74e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-6.0.6.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting emoji==1.2.0 (from kss)\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pecab (from kss)\n",
      "  Downloading pecab-1.0.8.tar.gz (26.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: networkx in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from kss) (3.4.2)\n",
      "Collecting jamo (from kss)\n",
      "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting hangul-jamo (from kss)\n",
      "  Downloading hangul_jamo-1.0.1-py3-none-any.whl.metadata (899 bytes)\n",
      "Collecting tossi (from kss)\n",
      "  Downloading tossi-0.3.1.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting distance (from kss)\n",
      "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=6.0.2 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from kss) (6.0.2)\n",
      "Collecting unidecode (from kss)\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cmudict (from kss)\n",
      "  Downloading cmudict-1.1.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting koparadigm (from kss)\n",
      "  Downloading koparadigm-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting kollocate (from kss)\n",
      "  Downloading kollocate-0.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting bs4 (from kss)\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: numpy in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from kss) (2.2.6)\n",
      "Collecting pytest (from kss)\n",
      "  Downloading pytest-9.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: scipy in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from kss) (1.15.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from bs4->kss) (4.13.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from beautifulsoup4->bs4->kss) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from beautifulsoup4->bs4->kss) (4.15.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from cmudict->kss) (8.7.0)\n",
      "Collecting importlib-resources>=5 (from cmudict->kss)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from importlib-metadata>=5->cmudict->kss) (3.23.0)\n",
      "Collecting whoosh (from kollocate->kss)\n",
      "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting xlrd==1.2.0 (from koparadigm->kss)\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pyarrow in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from pecab->kss) (21.0.0)\n",
      "Requirement already satisfied: regex in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from pecab->kss) (2025.8.29)\n",
      "Requirement already satisfied: exceptiongroup>=1 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from pytest->kss) (1.3.0)\n",
      "Collecting iniconfig>=1.0.1 (from pytest->kss)\n",
      "  Downloading iniconfig-2.3.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: packaging>=22 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from pytest->kss) (25.0)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->kss)\n",
      "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from pytest->kss) (2.19.2)\n",
      "Requirement already satisfied: tomli>=1 in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from pytest->kss) (2.2.1)\n",
      "Collecting bidict (from tossi->kss)\n",
      "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: six in /home/user/anaconda3/envs/ji_env/lib/python3.10/site-packages (from tossi->kss) (1.17.0)\n",
      "Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading cmudict-1.1.2-py3-none-any.whl (939 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m939.7/939.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading hangul_jamo-1.0.1-py3-none-any.whl (4.4 kB)\n",
      "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading kollocate-0.0.2-py3-none-any.whl (72.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading koparadigm-0.10.0-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "Downloading pytest-9.0.1-py3-none-any.whl (373 kB)\n",
      "Downloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
      "Downloading iniconfig-2.3.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
      "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "Downloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
      "Building wheels for collected packages: kss, distance, pecab, tossi\n",
      "\u001b[33m  DEPRECATION: Building 'kss' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'kss'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for kss (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kss: filename=kss-6.0.6-cp310-cp310-linux_x86_64.whl size=1132200 sha256=bc0b8f1d4302782496ba1677d1664fc6fc1f8e4dac1e261964b371f5e34ed69e\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/3c/93/14/5ca6fce8bdd0ab9d9b8f8ccd75159157a39dba9ff20c6db239\n",
      "\u001b[33m  DEPRECATION: Building 'distance' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'distance'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for distance (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for distance: filename=distance-0.1.3-py3-none-any.whl size=16321 sha256=1e9e546e2e4c9dc50a1589131abcdc8255578f9fe1b21bf6a33987707bfbd6e1\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
      "\u001b[33m  DEPRECATION: Building 'pecab' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pecab'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for pecab (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646750 sha256=b03bcad38dd57126d3d7b6eaa99d1a02b9b2eb4ff3c1a25f7e6169924a119fd7\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
      "\u001b[33m  DEPRECATION: Building 'tossi' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'tossi'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for tossi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tossi: filename=tossi-0.3.1-py3-none-any.whl size=12181 sha256=6e1dd08d8173362e634120e395fbc5efe56b7d7beef19029049cc9f55018bfc7\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/a7/18/60/1094a6fe93c8063efcd3e6700d09328216682e495a3c51af9f\n",
      "Successfully built kss distance pecab tossi\n",
      "Installing collected packages: whoosh, jamo, hangul-jamo, emoji, distance, xlrd, unidecode, pluggy, kollocate, iniconfig, importlib-resources, bidict, tossi, pytest, koparadigm, cmudict, bs4, pecab, kss\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19/19\u001b[0m [kss]32m18/19\u001b[0m [kss]b]]te]o]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bidict-0.23.1 bs4-0.0.2 cmudict-1.1.2 distance-0.1.3 emoji-1.2.0 hangul-jamo-1.0.1 importlib-resources-6.5.2 iniconfig-2.3.0 jamo-0.4.1 kollocate-0.0.2 koparadigm-0.10.0 kss-6.0.6 pecab-1.0.8 pluggy-1.6.0 pytest-9.0.1 tossi-0.3.1 unidecode-1.4.0 whoosh-2.7.4 xlrd-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df485194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 01_bridge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [01:02<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 02_earth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [03:09<00:00,  9.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 03_tunnel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:42<00:00,  6.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 04_scaffold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [01:02<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 05_crane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [02:24<00:00, 16.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 06_finishing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:30<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 07_concrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:22<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Processing DB: 08_general\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [01:10<00:00,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ Korean SafetyDocs Chunking (í‘œ+BR ì™„ì „íŒ) ì™„ë£Œ!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import kss\n",
    "from typing import List\n",
    "\n",
    "DB_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_md\"\n",
    "OUT_ROOT = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/FinalDB_chunks_fix\"\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "HEADER_PATTERN = r\"^(#{1,3})\\s*(\\d+(?:\\.\\d+)*)?\\s*(.*)$\"\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 0) HTML / br / table ì •ë¦¬\n",
    "# -------------------------------------------------------\n",
    "def clean_html(text: str) -> str:\n",
    "    # <br> â†’ \\n\n",
    "    text = text.replace(\"<br>\", \"\\n\")\n",
    "\n",
    "    # </td>, </tr> â†’ \\n ê°•ì œê°œí–‰\n",
    "    text = re.sub(r\"</td>|</tr>\", \"\\n\", text)\n",
    "\n",
    "    # <td>, <tr> ë“±ì€ ê·¸ëƒ¥ ì œê±°\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1) í—¤ë” íŒŒì‹±\n",
    "# -------------------------------------------------------\n",
    "def extract_headers(text):\n",
    "    headers = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        m = re.match(HEADER_PATTERN, line.strip())\n",
    "        if m:\n",
    "            hashes, number, title = m.groups()\n",
    "            depth = len(hashes)\n",
    "            headers.append({\n",
    "                \"depth\": depth,\n",
    "                \"number\": number.strip() if number else None,\n",
    "                \"title\": title.strip(),\n",
    "                \"text\": line.strip(),\n",
    "            })\n",
    "    return headers\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) Section ë‹¨ìœ„ split\n",
    "# -------------------------------------------------------\n",
    "def split_sections(text):\n",
    "    pattern = r\"(?=^#{1,3}\\s*)\"\n",
    "    sections = re.split(pattern, text, flags=re.MULTILINE)\n",
    "    cleaned = [s.strip() for s in sections if s.strip().startswith(\"#\")]\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) Section header\n",
    "# -------------------------------------------------------\n",
    "def parse_section_header(sec_text):\n",
    "    first_line = sec_text.split(\"\\n\")[0].strip()\n",
    "    m = re.match(HEADER_PATTERN, first_line)\n",
    "    if not m:\n",
    "        return None, None, None\n",
    "\n",
    "    hashes, number, title = m.groups()\n",
    "    depth = len(hashes)\n",
    "    heading = f\"{number} {title}\".strip() if number else title\n",
    "    return heading, number, depth\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) ê³„ì¸µ êµ¬ì¡°\n",
    "# -------------------------------------------------------\n",
    "def build_hierarchy(all_headers, current_number, current_depth):\n",
    "    if not current_number:\n",
    "        return []\n",
    "    parts = current_number.split(\".\")\n",
    "    hierarchy = []\n",
    "    for h in all_headers:\n",
    "        if not h[\"number\"]:\n",
    "            continue\n",
    "        h_parts = h[\"number\"].split(\".\")\n",
    "        if len(h_parts) <= len(parts) and h_parts == parts[:len(h_parts)]:\n",
    "            hierarchy.append(f\"{h['number']} {h['title']}\")\n",
    "    return hierarchy\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) bullet / ë¬¸ë‹¨ ë³‘í•©\n",
    "# -------------------------------------------------------\n",
    "BULLET_REGEX = r\"^(\\(|\\d+\\)|\\d+\\.)|^[\\-â€¢â–ª]|^o \"\n",
    "\n",
    "def normalize_paragraphs(text: str) -> List[str]:\n",
    "    lines = text.split(\"\\n\")\n",
    "    merged = []\n",
    "    buffer = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        l = line.strip()\n",
    "        if not l:\n",
    "            continue\n",
    "\n",
    "        if re.match(BULLET_REGEX, l):  # bullet start\n",
    "            if buffer:\n",
    "                merged.append(buffer)\n",
    "                buffer = \"\"\n",
    "            merged.append(l)\n",
    "        else:\n",
    "            if buffer:\n",
    "                buffer += \" \" + l\n",
    "            else:\n",
    "                buffer = l\n",
    "\n",
    "    if buffer:\n",
    "        merged.append(buffer)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) Sentence-aware chunking + overflow cut\n",
    "# -------------------------------------------------------\n",
    "def chunk_sentences(paragraphs, chunk_size=400, overlap=80):\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_len = 0\n",
    "\n",
    "    for para in paragraphs:\n",
    "        # í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬\n",
    "        sents = kss.split_sentences(para)\n",
    "\n",
    "        for sent in sents:\n",
    "            sent_len = len(sent)\n",
    "\n",
    "            # ë¬¸ì¥ ìì²´ê°€ chunk_sizeë³´ë‹¤ í¬ë©´ ê°•ì œ split\n",
    "            if sent_len > chunk_size:\n",
    "                forced = [sent[i:i+chunk_size] for i in range(0, sent_len, chunk_size)]\n",
    "                for f in forced:\n",
    "                    if current:\n",
    "                        chunks.append(\" \".join(current))\n",
    "                        current = []\n",
    "                        current_len = 0\n",
    "                    chunks.append(f.strip())\n",
    "                continue\n",
    "\n",
    "            # ì¼ë°˜ ë¬¸ì¥ ì²˜ë¦¬\n",
    "            if current_len + sent_len > chunk_size:\n",
    "                chunks.append(\" \".join(current))\n",
    "\n",
    "                # overlap ì ìš©\n",
    "                overlap_sents = []\n",
    "                acc = 0\n",
    "                for s in reversed(current):\n",
    "                    if acc + len(s) < overlap:\n",
    "                        overlap_sents.insert(0, s)\n",
    "                        acc += len(s)\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                current = overlap_sents.copy()\n",
    "                current_len = sum(len(s) for s in current)\n",
    "\n",
    "            current.append(sent)\n",
    "            current_len += sent_len\n",
    "\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7) ë©”ì¸ ì‘ì—…\n",
    "# -------------------------------------------------------\n",
    "for db_name in sorted(os.listdir(DB_ROOT)):\n",
    "    db_path = os.path.join(DB_ROOT, db_name)\n",
    "    if not os.path.isdir(db_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“ Processing DB: {db_name}\")\n",
    "\n",
    "    out_folder = os.path.join(OUT_ROOT, db_name)\n",
    "    os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "    out_path = os.path.join(out_folder, f\"{db_name}_chunks.jsonl\")\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for file_name in tqdm(os.listdir(db_path)):\n",
    "            if not file_name.endswith(\".md\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(db_path, file_name)\n",
    "            raw_text = open(file_path, encoding=\"utf-8\").read()\n",
    "\n",
    "            # âœ¨ HTML/í‘œ/BR ì •ë¦¬\n",
    "            raw_text = clean_html(raw_text)\n",
    "\n",
    "            all_headers = extract_headers(raw_text)\n",
    "            sections = split_sections(raw_text)\n",
    "\n",
    "            for sec in sections:\n",
    "                section_heading, section_number, depth = parse_section_header(sec)\n",
    "                if not section_heading:\n",
    "                    continue\n",
    "\n",
    "                hierarchy_list = build_hierarchy(all_headers, section_number, depth)\n",
    "                hierarchy_str = \" > \".join(hierarchy_list)\n",
    "\n",
    "                body = sec.split(\"\\n\", 1)[1] if \"\\n\" in sec else \"\"\n",
    "\n",
    "                paragraphs = normalize_paragraphs(body)\n",
    "                chunks = chunk_sentences(paragraphs)\n",
    "\n",
    "                for idx, chunk in enumerate(chunks):\n",
    "                    record = {\n",
    "                        \"db\": db_name,\n",
    "                        \"file\": file_name,\n",
    "                        \"section\": section_heading,\n",
    "                        \"section_number\": section_number,\n",
    "                        \"hierarchy\": hierarchy_list,\n",
    "                        \"hierarchy_str\": hierarchy_str,\n",
    "                        \"chunk_id\": idx,\n",
    "                        \"source_path\": file_path,\n",
    "                        \"content\": chunk.strip(),\n",
    "                    }\n",
    "                    out_f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"\\nğŸ‰ Korean SafetyDocs Chunking (í‘œ+BR ì™„ì „íŒ) ì™„ë£Œ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "DESCRIPTIONS = {\n",
    "    \"01_bridge\": {\n",
    "        \"name\": \"Bridge Construction Safety DB\",\n",
    "        \"domain\": \"êµëŸ‰ê³µì‚¬\",\n",
    "        \"purpose\": \"êµëŸ‰ ì‹œê³µê³¼ ê´€ë ¨ëœ ëª¨ë“  ê³µì •(FCM, ILM, PSM, PCTê±°ë” ë“±)ì— ëŒ€í•œ ì•ˆì „ì‘ì—… ì§€ì¹¨ê³¼ ì‚¬ê³  ì˜ˆë°© ê¸°ì¤€ì„ ì œê³µí•œë‹¤.\",\n",
    "        \"covers\": [\"êµëŸ‰ ìƒë¶€ê³µ\", \"ê±°ë” ì„¤ì¹˜\", \"ìŠ¬ë˜ë¸Œ ê±°í‘¸ì§‘\", \"í˜„ìˆ˜êµ ì‹œê³µ\"],\n",
    "        \"common_accidents\": [\"ê±°í‘¸ì§‘ ë¶•ê´´\", \"ì¶”ë½\", \"ë‚™í•˜ë¬¼\"],\n",
    "        \"best_for_queries\": [\"êµëŸ‰ ê±°í‘¸ì§‘ ë¶•ê´´\", \"ê±°ë” ì¸ì–‘ ì‚¬ê³ \"]\n",
    "    },\n",
    "    \"02_earth\": {\n",
    "        \"name\": \"Earthwork & Excavation Safety DB\",\n",
    "        \"domain\": \"í† ê³µì‚¬/êµ´ì°©\",\n",
    "        \"purpose\": \"êµ´ì°©ê³µì‚¬, í™ë§‰ì´ ì§€ë³´ê³µ, SCW/CIP ë“± ì§€ë°˜ ê´€ë ¨ ê³µì •ì˜ ë¶•ê´´Â·ë§¤ëª° ì‚¬ê³  ì˜ˆë°© ì§€ì¹¨ ì œê³µ.\",\n",
    "        \"covers\": [\"í„°íŒŒê¸°\", \"í™ë§‰ì´\", \"SCW\", \"CIP\"],\n",
    "        \"common_accidents\": [\"ë¶•ê´´\", \"ë§¤ëª°\"],\n",
    "        \"best_for_queries\": [\"í† ì‚¬ ë¶•ê´´\", \"í™ë§‰ì´ ë³€í˜• ì‚¬ê³ \"]\n",
    "    },\n",
    "    \"03_tunnel\": {\n",
    "        \"name\": \"Tunnel Construction Safety DB\",\n",
    "        \"domain\": \"í„°ë„\",\n",
    "        \"purpose\": \"NATM, TBM, ë°œíŒŒ ë“± í„°ë„ êµ´ì°© ê´€ë ¨ ì•ˆì „ì§€ì¹¨ ì œê³µ.\",\n",
    "        \"covers\": [\"ë°œíŒŒ\", \"ìˆí¬ë¦¬íŠ¸\", \"ë¡ë³¼íŠ¸\", \"ì§€ë³´ê³µ\"],\n",
    "        \"common_accidents\": [\"ë‚™ì„\", \"ë¶•ë½\", \"ê°€ìŠ¤ í­ë°œ\"],\n",
    "        \"best_for_queries\": [\"í„°ë„ ë¶•ë½ ì‚¬ê³ \", \"ë°œíŒŒ ì‘ì—… ì‚¬ê³ \"]\n",
    "    },\n",
    "    \"04_scaffold\": {\n",
    "        \"name\": \"Scaffolding Safety DB\",\n",
    "        \"domain\": \"ë¹„ê³„/ê°€ì„¤\",\n",
    "        \"purpose\": \"ë¹„ê³„, ë‹¬ë¹„ê³„, ì´ë™ì‹ ë¹„ê³„ ë“± ê³ ì†Œì‘ì—… ì•ˆì „ê¸°ì¤€ ì œê³µ.\",\n",
    "        \"covers\": [\"ë¹„ê³„ ì„¤ì¹˜\", \"ë‹¬ë¹„ê³„\", \"ì´ë™ì‹ ë¹„ê³„\"],\n",
    "        \"common_accidents\": [\"ì¶”ë½\", \"ë¹„ê³„ ë¶•ê´´\"],\n",
    "        \"best_for_queries\": [\"ë¹„ê³„ ì¶”ë½ ì‚¬ê³ \"]\n",
    "    },\n",
    "    \"05_crane\": {\n",
    "        \"name\": \"Crane & Lifting Safety DB\",\n",
    "        \"domain\": \"íƒ€ì›Œí¬ë ˆì¸/ì¸ì–‘\",\n",
    "        \"purpose\": \"íƒ€ì›Œí¬ë ˆì¸ ë° ì¤‘ëŸ‰ë¬¼ ì¸ì–‘ ì‘ì—… ì•ˆì „ì§€ì¹¨ ì œê³µ.\",\n",
    "        \"covers\": [\"íƒ€ì›Œí¬ë ˆì¸\", \"ì´ë™ì‹ í¬ë ˆì¸\"],\n",
    "        \"common_accidents\": [\"ì „ë„\", \"ë‚™í•˜\", \"ë¡œí”„ íŒŒë‹¨\"],\n",
    "        \"best_for_queries\": [\"í¬ë ˆì¸ ì „ë„\", \"ì¸ì–‘ë¬¼ ë‚™í•˜\"]\n",
    "    },\n",
    "    \"06_finishing\": {\n",
    "        \"name\": \"Finishing Construction Safety DB\",\n",
    "        \"domain\": \"ë§ˆê°\",\n",
    "        \"purpose\": \"ì‹¤ë‚´ ë§ˆê°ê³µì‚¬ ì•ˆì „ì§€ì¹¨ ì œê³µ.\",\n",
    "        \"covers\": [\"ì„ê³ ë³´ë“œ\", \"ì°½í˜¸\", \"ë‚´ë¶€ ë§ˆê°\"],\n",
    "        \"common_accidents\": [\"ì‚¬ë‹¤ë¦¬ ì „ë„\", \"ë‚™í•˜\"],\n",
    "        \"best_for_queries\": [\"ì‹¤ë‚´ ì‚¬ë‹¤ë¦¬ ì‚¬ê³ \"]\n",
    "    },\n",
    "    \"07_concrete\": {\n",
    "        \"name\": \"Concrete & Formwork Safety DB\",\n",
    "        \"domain\": \"ì½˜í¬ë¦¬íŠ¸\",\n",
    "        \"purpose\": \"íƒ€ì„¤, ê±°í‘¸ì§‘, ë™ë°”ë¦¬ ì‘ì—… ì•ˆì „ê¸°ì¤€ ì œê³µ.\",\n",
    "        \"covers\": [\"ê±°í‘¸ì§‘\", \"ë™ë°”ë¦¬\", \"íƒ€ì„¤\"],\n",
    "        \"common_accidents\": [\"ê±°í‘¸ì§‘ ë¶•ê´´\", \"ë™ë°”ë¦¬ ì¢Œêµ´\"],\n",
    "        \"best_for_queries\": [\"ë™ë°”ë¦¬ ë¶•ê´´\", \"íƒ€ì„¤ ì‚¬ê³ \"]\n",
    "    },\n",
    "    \"08_general\": {\n",
    "        \"name\": \"General Construction Safety DB\",\n",
    "        \"domain\": \"ê³µí†µ ì•ˆì „\",\n",
    "        \"purpose\": \"ëª¨ë“  ê³µì¢…ì—ì„œ ê³µí†µì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ì•ˆì „ ê¸°ì¤€ ì œê³µ.\",\n",
    "        \"covers\": [\"PPE\", \"í˜„ì¥ ì•ˆì „ ìˆ˜ì¹™\"],\n",
    "        \"common_accidents\": [\"ì¶”ë½\", \"ë‚™í•˜\"],\n",
    "        \"best_for_queries\": [\"í˜„ì¥ ì•ˆì „ìˆ˜ì¹™\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "BASE_PATH = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/DB\"\n",
    "\n",
    "for folder, desc in DESCRIPTIONS.items():\n",
    "    folder_path = os.path.join(BASE_PATH, folder)\n",
    "    output_path = os.path.join(folder_path, \"description.json\")\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"âŒ í´ë” ì—†ìŒ: {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(desc, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… ìƒì„±ë¨: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f46ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# CSV ê²½ë¡œ\n",
    "csv_path = \"/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/data/test_preprocessing.csv\"\n",
    "\n",
    "# CSV ë¡œë“œ\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ë°œìƒì¼ì‹œ ì»¬ëŸ¼ parsing\n",
    "df[\"ë°œìƒì¼ì‹œ_parsed\"] = pd.to_datetime(df[\"ë°œìƒì¼ì‹œ\"].str.split().str[0], errors=\"coerce\")\n",
    "\n",
    "# ê¸°ì¤€ ë‚ ì§œ\n",
    "cutoff = pd.to_datetime(\"2024-12-10\")\n",
    "\n",
    "def update_year(row):\n",
    "    date = row[\"ë°œìƒì¼ì‹œ_parsed\"]\n",
    "    if pd.isna(date):\n",
    "        return row[\"ë°œìƒì¼ì‹œ\"]  # ë‚ ì§œ íŒŒì‹± ì•ˆ ë˜ë©´ ì›ë³¸ ìœ ì§€\n",
    "\n",
    "    # 2024-12-10 ì´í›„ â†’ ë³€ê²½ X\n",
    "    if date >= cutoff:\n",
    "        return row[\"ë°œìƒì¼ì‹œ\"]\n",
    "\n",
    "    # ê·¸ ì´ì „ ë‚ ì§œ â†’ ì—°ë„ë¥¼ 2025ë¡œ ë³€ê²½\n",
    "    new_date = date.replace(year=2025)\n",
    "    # ì‹œê°„ê¹Œì§€ í¬í•¨í•˜ê³  ìˆìœ¼ë©´ ê°™ì´ ìœ ì§€\n",
    "    time_part = \"\"\n",
    "    if \" \" in row[\"ë°œìƒì¼ì‹œ\"]:\n",
    "        time_part = row[\"ë°œìƒì¼ì‹œ\"].split(\" \", 1)[1]\n",
    "        return f\"{new_date.strftime('%Y-%m-%d')} {time_part}\"\n",
    "    else:\n",
    "        return new_date.strftime('%Y-%m-%d')\n",
    "\n",
    "# ìƒˆ ë‚ ì§œ ì ìš©\n",
    "df[\"ë°œìƒì¼ì‹œ\"] = df.apply(update_year, axis=1)\n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
    "df.drop(columns=[\"ë°œìƒì¼ì‹œ_parsed\"], inplace=True)\n",
    "\n",
    "# ì €ì¥\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"âœ… ì™„ë£Œ: ë‚ ì§œ ì—…ë°ì´íŠ¸ê°€ ëë‚¬ìŠµë‹ˆë‹¤!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiseok",
   "language": "python",
   "name": "ji_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

import os
import re
from typing import Dict, Any, List
from bs4 import BeautifulSoup
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain.retrievers.document_compressors import CrossEncoderReranker


# =====================================
# ğŸ”¹ Qwen3 4B Embedding ì„¤ì • ê·¸ëŒ€ë¡œ ìœ ì§€
# =====================================
def get_qwen_api_embeddings():
    embedder_model_name = "Qwen/Qwen3-Embedding-4B"
    embedder_base_url = "http://211.47.56.71:15653/v1"
    embedder_api_key = "token-abc123"
    
    print(f"ğŸŒ Qwen Embedding API ì—°ê²° ì¤‘: {embedder_base_url}")
    embeddings = OpenAIEmbeddings(
        model=embedder_model_name,
        base_url=embedder_base_url,
        api_key=embedder_api_key,
    )
    return embeddings


# =====================================
# ğŸ”¹ í…ìŠ¤íŠ¸ ì •ì œ ìœ í‹¸ í•¨ìˆ˜
# =====================================
def _prettify_text(text: str) -> str:
    text = re.sub(r"[\u2027â€¢â€¤Â·]+", "Â·", text)
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"(\.)([ê°€-í£])", r"\1\n\2", text)
    text = re.sub(r"(Â·\s*)", r"\n- ", text)
    text = re.sub(r"([ê°€-í£])(\s*:\s*)", r"\1\n", text)
    text = text.strip()
    return text


def _clean_html(text: str) -> str:
    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text(separator=" ", strip=True)
    text = re.sub(r"<[^>]+>", "", text)
    text = re.sub(r"(í‘œ\s*\d+|ë¶€ë¡\s*\d+|ë¶€ë¡í‘œ\s*\d+)", "", text)
    return _prettify_text(text)


def _is_table_heavy(text: str) -> bool:
    return text.count("|") > 5 or text.count("<table") > 0 or len(text.split()) < 30


def _is_noise_section(doc: Document) -> bool:
    noise_keywords = ["ë¶€ë¡", "ì ê²€", "í™•ì¸ì‚¬í•­", "í•­íƒ€ê¸°", "í•­ë°œê¸°", "ì ê²€í‘œ"]
    section = doc.metadata.get("section", "")
    text = doc.page_content
    return any(k in section or k in text[:200] for k in noise_keywords)


# =====================================
# ğŸ”¹ Retriever ë³¸ì²´
# =====================================
class RerankRetriever:
    EXCLUDED_SECTIONS = {
        '1. ëª©ì ', '1. ëª© ì ', '2. ì ìš©ë²”ìœ„', '3. ìš©ì–´ì˜ì •ì˜', '3. ì •ì˜',
        'í•œêµ­ì‚°ì—…ì•ˆì „ë³´ê±´ê³µë‹¨', 'ì•ˆì „ë³´ê±´ê¸°ìˆ ì§€ì¹¨ì˜ê°œìš”', 'ì§€ì¹¨ê°œì •ì´ë ¥',
        'â—‹ì œì •ê²½ê³¼', 'ì œì •ê²½ê³¼', 'ê°œì •ì´ë ¥',
        'â—‹ê¸°ìˆ ì§€ì¹¨ì˜ì ìš©ë°ë¬¸ì˜', 'â—‹ê´€ë ¨ë²•ê·œâ€¤ê·œì¹™â€¤ê³ ì‹œë“±'
    }

    EXCLUDED_SECTION_KEYWORDS = [
        'ëª©ì ', 'ì •ì˜', 'ì ìš©ë²”ìœ„', 'ì´ì¹™', 'ê°œìš”', 'ì¼ë°˜ì‚¬í•­',
        'ì œì •ê²½ê³¼', 'ê°œì •ì´ë ¥', 'ì œì •ì', 'ê³µí‘œì¼ì',
    ]

    EXCLUDED_CONTENT_PATTERNS = [
        'ì´ ì§€ì¹¨ì€', 'ì´ ê·œì¹™ì€', 'ì´ ê¸°ì¤€ì˜ ëª©ì ', 'ìš©ì–´ì˜ ëœ»ì€', 'ìš©ì–´ì˜ ì •ì˜ëŠ”',
        'ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜', 'ì ìš©ë²”ìœ„ëŠ”', 'ì œì •ì:', 'ê³µí‘œì¼ì:', 'ê°œì •ì¼ì:',
        'ì•ˆì „ë³´ê±´ê¸°ìˆ ì§€ì¹¨ì€', 'www.kosha.or.kr', 'í•œêµ­ì‚°ì—…ì•ˆì „ë³´ê±´ê³µë‹¨ì´ì‚¬ì¥',
        'ì œì •', 'ê°œì •', '2010ë…„', '2012ë…„', '2020ë…„',
    ]

    def __init__(
        self,
        title_db_path: str,
        content_db_path: str,
        reranker_model: str = "BAAI/bge-reranker-v2-m3",
        title_top_k: int = 5,
        contents_top_k: int = 8,
        alpha: float = 0.5,  # âœ… dense/sparse ë¹„ìœ¨
        min_content_length: int = 100,
    ):
        self.title_db_path = title_db_path
        self.content_db_path = content_db_path
        self.reranker_model = reranker_model
        self.title_top_k = title_top_k
        self.contents_top_k = contents_top_k
        self.alpha = alpha
        self.min_content_length = min_content_length

        self.title_db = None
        self.content_db = None

        print(f"ğŸ” RerankRetriever ì´ˆê¸°í™” ì¤‘ (dense:sparse={self.alpha}:{1-self.alpha})")
        self._setup()
        print("âœ… RerankRetriever ìƒì„± ì™„ë£Œ")

    # =====================================
    # ğŸ“˜ ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
    # =====================================
    def _setup(self):
        embeddings = get_qwen_api_embeddings()
        if not os.path.exists(self.title_db_path):
            raise FileNotFoundError(f"âŒ Title DB ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.title_db_path}")
        if not os.path.exists(self.content_db_path):
            raise FileNotFoundError(f"âŒ Content DB ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.content_db_path}")

        print(f"ğŸ“š Title DB ë¡œë“œ ì¤‘: {self.title_db_path}")
        self.title_db = FAISS.load_local(self.title_db_path, embeddings, allow_dangerous_deserialization=True)
        print(f"ğŸ“– Content DB ë¡œë“œ ì¤‘: {self.content_db_path}")
        self.content_db = FAISS.load_local(self.content_db_path, embeddings, allow_dangerous_deserialization=True)

    # =====================================
    # âš™ï¸ ë¬¸ì„œ í•„í„°ë§
    # =====================================
    def _is_excluded_document(self, doc: Document) -> bool:
        section = doc.metadata.get('section', '').strip()
        content = doc.page_content.strip()
        if section in self.EXCLUDED_SECTIONS:
            return True
        if any(kw in section.lower() for kw in self.EXCLUDED_SECTION_KEYWORDS):
            return True
        if any(p in content[:200] for p in self.EXCLUDED_CONTENT_PATTERNS):
            return True
        if len(content) < self.min_content_length:
            return True
        return False

    # =====================================
    # ğŸ” Title DB í•„í„°ë§
    # =====================================
    def _filter_by_title(self, query: str) -> List[str]:
        print(f"\nğŸ” [STAGE 1] Title DB í•„í„°ë§... (top_k={self.title_top_k})")
        title_docs = self.title_db.similarity_search(query, k=self.title_top_k)
        filtered = list({d.metadata.get("source", "") for d in title_docs if d.metadata.get("source")})
        print(f"âœ… í•„í„°ë§ëœ íŒŒì¼: {len(filtered)}ê°œ")
        for i, f in enumerate(filtered, 1):
            print(f"   [{i}] {f}")
        return filtered

    # =====================================
    # ğŸ§© Dense/Sparse Hybrid Merge (ê°€ì¤‘ì¹˜ ì ìš©)
    # =====================================
    def _hybrid_merge(self, dense_results, sparse_results) -> List[Document]:
        dense_dict = {hash(doc.page_content): score for doc, score in dense_results}
        sparse_dict = {hash(doc.page_content): i for i, doc in enumerate(sparse_results)}

        all_docs = []
        for doc, d_score in dense_results:
            h = hash(doc.page_content)
            s_rank = sparse_dict.get(h, len(sparse_results))
            combined_score = self.alpha * d_score + (1 - self.alpha) * (1 - s_rank / len(sparse_results))
            all_docs.append((doc, combined_score))

        # sparse only ì¶”ê°€
        for i, doc in enumerate(sparse_results):
            h = hash(doc.page_content)
            if h not in dense_dict:
                combined_score = (1 - self.alpha) * (1 - i / len(sparse_results))
                all_docs.append((doc, combined_score))

        all_docs = sorted(all_docs, key=lambda x: x[1], reverse=True)
        return [doc for doc, _ in all_docs]

    # =====================================
    # ğŸ“– Content DB ê²€ìƒ‰
    # =====================================
    def _retrieve_from_content_db(self, query: str, files: List[str]) -> List[Document]:
        print(f"\nğŸ“„ [STAGE 2] Content DBì—ì„œ íŒŒì¼ ê²€ìƒ‰...")

        all_docs = list(self.content_db.docstore._dict.values())
        filtered_docs = [d for d in all_docs if d.metadata.get("source") in files and not self._is_excluded_document(d)]
        if not filtered_docs:
            filtered_docs = all_docs

        dense_results = self.content_db.similarity_search_with_score(query, k=self.contents_top_k * 4)
        sparse_retriever = BM25Retriever.from_documents(filtered_docs)
        sparse_retriever.k = self.contents_top_k * 4
        sparse_results = sparse_retriever.get_relevant_documents(query)

        print(f"ğŸ“Š Dense/Sparse ê²°í•© ì¤‘... (alpha={self.alpha})")
        hybrid_docs = self._hybrid_merge(dense_results, sparse_results)

        # reranker
        cross_encoder = HuggingFaceCrossEncoder(model_name=self.reranker_model)
        compressor = CrossEncoderReranker(model=cross_encoder, top_n=self.contents_top_k * 2)
        reranked = compressor.compress_documents(hybrid_docs, query)

        # âœ… ì •ì œ ë° í•„í„°ë§
        cleaned = []
        for d in reranked:
            d.page_content = _clean_html(d.page_content)
            if (
                len(d.page_content) > self.min_content_length
                and not _is_table_heavy(d.page_content)
                and not _is_noise_section(d)
                and not self._is_excluded_document(d)
            ):
                cleaned.append(d)

        print(f"âœ… ìµœì¢… í•„í„°ë§ í›„ {len(cleaned)}ê°œ ë¬¸ì„œ ìœ ì§€")
        return cleaned[: self.contents_top_k]

    # =====================================
    # ğŸš€ ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸
    # =====================================
    def retrieve(self, query: str) -> List[Document]:
        print(f"\n{'='*80}\nğŸ“ ì…ë ¥ ì¿¼ë¦¬: {query}\n{'='*80}")
        lines = query.splitlines()
        construct = next((l.split(":")[1].strip() for l in lines if "ê³µì¢…" in l), None)
        process = next((l.split(":")[1].strip() for l in lines if "ì‘ì—…í”„ë¡œì„¸ìŠ¤" in l), None)
        core_query = process or construct or query
        print(f"ğŸ¯ í•µì‹¬ ê²€ìƒ‰ì–´: {core_query}")

        files = self._filter_by_title(core_query)
        docs = self._retrieve_from_content_db(core_query, files)

        print(f"\nâœ… ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ\n" + "="*80)
        return docs


# =====================================
# ğŸ”¹ ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
# =====================================
retriever_instance = RerankRetriever(
    title_db_path="/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/DB/title_db",
    content_db_path="/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/DB/content_db",
    reranker_model="BAAI/bge-reranker-v2-m3",
    title_top_k=5,
    contents_top_k=8,
    alpha=0.3,  # âœ… dense:sparse 1:1
    min_content_length=100,
)


# =====================================
# ğŸ”¹ (ì„ íƒì ) LangGraphìš© Node í•¨ìˆ˜ - í˜¸í™˜ìš©
# =====================================
def retrieve_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì˜ˆì „ LangGraph êµ¬ì¡°ì™€ì˜ í˜¸í™˜ì„ ìœ„í•œ node í•¨ìˆ˜.
    í˜„ì¬ëŠ” Orchestrator + RAGAgent êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë‚¨ê²¨ë‘ .
    """
    query = state.get("user_query") or state.get("query", "")
    docs = retriever_instance.retrieve(query)

    docs_text = "\n\n".join(
        f"[{i+1}] ({d.metadata.get('source','?')} - {d.metadata.get('section','?')})\n{d.page_content}"
        for i, d in enumerate(docs)
    )
    sources = [
        {"idx": i + 1, "filename": d.metadata.get("source", ""), "section": d.metadata.get("section", "")}
        for i, d in enumerate(docs)
    ]

    state["retrieved_docs"] = docs
    state["docs_text"] = docs_text
    state["sources"] = sources

    return state

"""
Phase 3: ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬ (LLM Factory ì ìš© + ë¹„ë™ê¸° ainvoke ì‚¬ìš©)

âœ… ìˆ˜ì •ì‚¬í•­:
1. LLM Factory (Qwen-Fast) ì ìš©ìœ¼ë¡œ ë¹„ìš© ì ˆê°.
2. cl.make_async(call_llm) ëŒ€ì‹  LangChainì˜ ainvoke() ì‚¬ìš©ìœ¼ë¡œ ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”.
"""

from typing import List, Dict, Any
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
import json
import chainlit as cl

# âœ… Factory Import
from core.llm_factory import get_llm

# ======================================================================
# 1. Pydantic ëª¨ë¸ ì •ì˜ (ì¶œë ¥ êµ¬ì¡°í™”)
# ======================================================================
class DuplicateCheck(BaseModel):
    is_duplicate: bool = Field(description="ì¤‘ë³µ ì—¬ë¶€ (true/false)")

class KeyInfoExtraction(BaseModel):
    relevance_summary: str = Field(description="ë¬¸ì„œì™€ ì‚¬ê³ ì˜ ê´€ë ¨ì„± ìš”ì•½ (í•œ ë¬¸ì¥)")
    key_sentences: List[str] = Field(description="í•µì‹¬ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 3ê°œ)")


# ======================================================================
# 2. AdvancedDocumentProcessor í´ë˜ìŠ¤
# ======================================================================
class AdvancedDocumentProcessor:
    """ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬: ì¤‘ë³µ ì œê±° & í•µì‹¬ ì¶”ì¶œ"""
    
    def __init__(self):
        self.similarity_threshold = 0.85
        # âœ… Qwen(Fast) ëª¨ë¸ ì‚¬ìš© (ë¬¸ì„œ ì²˜ë¦¬ëŠ” ë¡œì»¬ë¡œ ì¶©ë¶„)
        self.llm = get_llm(mode="fast")
    
    async def process_documents( 
        self, 
        docs: List[Document], 
        user_query: str,
        remove_duplicates: bool = True,
        extract_key_sentences: bool = True
    ) -> List[Dict[str, Any]]:
        """
        ë¬¸ì„œ ê³ ê¸‰ ì²˜ë¦¬ (ë¹„ë™ê¸°)
        """
        if not docs:
            return []
        
        processed_docs = []
        print(f"\nğŸ” ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ ({len(docs)}ê°œ ë¬¸ì„œ)")
        
        # 1ë‹¨ê³„: ì¤‘ë³µ ì œê±°
        if remove_duplicates:
            print("\nğŸ“Š 1ë‹¨ê³„: ë¬¸ì„œ ê°„ ì¤‘ë³µ ì œê±° ì¤‘...")
            unique_docs = await self._remove_duplicates_llm(docs)
            print(f"   âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(docs)}ê°œ â†’ {len(unique_docs)}ê°œ")
        else:
            unique_docs = docs
        
        # 2ë‹¨ê³„: ê° ë¬¸ì„œ ì²˜ë¦¬
        print("\nğŸ“ 2ë‹¨ê³„: í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì¤‘...")
        for idx, doc in enumerate(unique_docs, 1):
            # print(f"   ì²˜ë¦¬ ì¤‘... [{idx}/{len(unique_docs)}]", end='\r')
            
            result = {
                "doc": doc,
                "is_duplicate": False,
                "key_sentences": [],
                "relevance_summary": ""
            }
            
            # í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
            if extract_key_sentences:
                key_info = await self._extract_key_info_llm(doc.page_content, user_query)
                result["key_sentences"] = key_info.get("key_sentences", [])
                result["relevance_summary"] = key_info.get("relevance_summary", "")
            
            processed_docs.append(result)
        
        print(f"\n   âœ… í•µì‹¬ ì¶”ì¶œ ì™„ë£Œ: {len(processed_docs)}ê°œ ë¬¸ì„œ")
        
        return processed_docs
    
    async def _remove_duplicates_llm(self, docs: List[Document]) -> List[Document]:
        """LLM ê¸°ë°˜ ì¤‘ë³µ ë¬¸ì„œ ì œê±° (ë¹„ë™ê¸°)"""
        if len(docs) <= 1:
            return docs
        
        unique_docs = [docs[0]]  # ì²« ë²ˆì§¸ëŠ” í•­ìƒ í¬í•¨
        
        for idx, new_doc in enumerate(docs[1:], 2):
            # ê¸°ì¡´ ë¬¸ì„œë“¤ê³¼ ë¹„êµ
            is_duplicate = await self._check_duplicate_with_llm(new_doc, unique_docs)
            
            if not is_duplicate:
                unique_docs.append(new_doc)
        
        return unique_docs
    
    async def _check_duplicate_with_llm(self, new_doc: Document, existing_docs: List[Document]) -> bool:
        """ìƒˆ ë¬¸ì„œê°€ ê¸°ì¡´ ë¬¸ì„œë“¤ê³¼ ì¤‘ë³µë˜ëŠ”ì§€ LLMìœ¼ë¡œ íŒë‹¨ (ë¹„ë™ê¸°)"""
        
        existing_summaries = []
        for doc in existing_docs[-3:]: # ìµœê·¼ 3ê°œë§Œ ë¹„êµ (ì†ë„ ìµœì í™”)
            metadata = doc.metadata
            summary = f"íŒŒì¼: {metadata.get('file', '?')}, ì„¹ì…˜: {metadata.get('section', '?')}"
            existing_summaries.append(summary)
        
        new_metadata = new_doc.metadata
        new_summary = f"íŒŒì¼: {new_metadata.get('file', '?')}, ì„¹ì…˜: {new_metadata.get('section', '?')}"
        
        system_template = """
ê¸°ì¡´ ë¬¸ì„œë“¤ (ìµœê·¼ 3ê°œ):
{existing_docs}

ìƒˆ ë¬¸ì„œ:
{new_doc}

íŒë‹¨: ìƒˆ ë¬¸ì„œê°€ ê¸°ì¡´ ë¬¸ì„œë“¤ê³¼ **ê°™ì€ ë‚´ìš©**ì¸ê°€?

íŒë‹¨ ê¸°ì¤€:
- ê°™ì€ íŒŒì¼ì˜ ê°™ì€ ì„¹ì…˜ â†’ ì¤‘ë³µ (true)
- ê°™ì€ íŒŒì¼ì˜ ë‹¤ë¥¸ ì„¹ì…˜ â†’ ë¹„ì¤‘ë³µ (false)
- ë‹¤ë¥¸ íŒŒì¼ â†’ ë¹„ì¤‘ë³µ (false)

JSON ì¶œë ¥ë§Œ:
{{ "is_duplicate": true/false }}
"""
        prompt = ChatPromptTemplate.from_messages([
            ("user", system_template)
        ])
        
        # Pydantic Parser ì‚¬ìš© (êµ¬ì¡°í™”ëœ ì¶œë ¥ ë³´ì¥)
        parser = JsonOutputParser(pydantic_object=DuplicateCheck)
        chain = prompt | self.llm | parser
        
        try:
            # âœ… ë¹„ë™ê¸° í˜¸ì¶œ (ainvoke)
            result = await chain.ainvoke({
                "existing_docs": "\n".join(f"- {s}" for s in existing_summaries),
                "new_doc": new_summary
            })
            return result.get("is_duplicate", False)
        
        except Exception as e:
            print(f"\nâš ï¸ ì¤‘ë³µ íŒë‹¨ ì‹¤íŒ¨: {e} (ë¹„ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼)")
            return False
    
    async def _extract_key_info_llm(self, content: str, user_query: str) -> Dict[str, Any]:
        """LLMìœ¼ë¡œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        
        system_template = """
ì‚¬ìš©ìê°€ ë‹¤ìŒ ì‚¬ê³ ë¥¼ ì¡°ì‚¬ ì¤‘ì…ë‹ˆë‹¤:
{user_query}

ë¬¸ì„œ ë‚´ìš©:
{content}

ì„ë¬´:
1. ì´ ë¬¸ì„œê°€ ì‚¬ê³ ì™€ ì–´ë–»ê²Œ ê´€ë ¨ë˜ëŠ”ì§€ **í•œ ë¬¸ì¥**ìœ¼ë¡œ ìš”ì•½
2. ì‚¬ê³  ì˜ˆë°©/ëŒ€ì‘ì— ë„ì›€ë˜ëŠ” **í•µì‹¬ ë¬¸ì¥ ìµœëŒ€ 3ê°œ** ì¶”ì¶œ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)

JSON ì¶œë ¥ í¬ë§·ì„ ì—„ìˆ˜í•˜ì„¸ìš”:
{{
    "relevance_summary": "ìš”ì•½ë¬¸",
    "key_sentences": ["ë¬¸ì¥1", "ë¬¸ì¥2", "ë¬¸ì¥3"]
}}
"""
        prompt = ChatPromptTemplate.from_messages([
            ("user", system_template)
        ])
        
        parser = JsonOutputParser(pydantic_object=KeyInfoExtraction)
        chain = prompt | self.llm | parser
        
        try:
            # âœ… ë¹„ë™ê¸° í˜¸ì¶œ (ainvoke)
            result = await chain.ainvoke({
                "user_query": user_query,
                "content": content[:2000] # í† í° ì ˆì•½ (ì•ë¶€ë¶„ë§Œ ì‚¬ìš©)
            })
            return result
        
        except Exception as e:
            print(f"\nâš ï¸ í•µì‹¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "relevance_summary": "ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨",
                "key_sentences": []
            }
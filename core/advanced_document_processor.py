"""
Phase 3: ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬

âœ… ìˆ˜ì •ì‚¬í•­: ëª¨ë“  LLM í˜¸ì¶œì„ cl.make_asyncë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë™ê¸°ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.
"""

from typing import List, Dict, Any
from langchain_core.documents import Document
from core.llm_utils import call_llm
import json
import chainlit as cl # cl.make_async ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€


class AdvancedDocumentProcessor:
    """ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬: ì¤‘ë³µ ì œê±° & í•µì‹¬ ì¶”ì¶œ"""
    
    def __init__(self):
        self.similarity_threshold = 0.85
    
    # ğŸŒŸ ë©”ì„œë“œ ì •ì˜: async ì¶”ê°€ ë° ë‚´ë¶€ await ì²˜ë¦¬
    async def process_documents( 
        self, 
        docs: List[Document], 
        user_query: str,
        remove_duplicates: bool = True,
        extract_key_sentences: bool = True
    ) -> List[Dict[str, Any]]:
        """
        ë¬¸ì„œ ê³ ê¸‰ ì²˜ë¦¬ (ë¹„ë™ê¸°)
        """
        
        if not docs:
            return []
        
        processed_docs = []
        
        print(f"\nğŸ” ê³ ê¸‰ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘ ({len(docs)}ê°œ ë¬¸ì„œ)")
        
        # 1ë‹¨ê³„: ì¤‘ë³µ ì œê±°
        if remove_duplicates:
            print("\nğŸ“Š 1ë‹¨ê³„: ë¬¸ì„œ ê°„ ì¤‘ë³µ ì œê±° ì¤‘...")
            unique_docs = await self._remove_duplicates_llm(docs) # ğŸŒŸ await ì¶”ê°€
            print(f" Â  âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(docs)}ê°œ â†’ {len(unique_docs)}ê°œ")
        else:
            unique_docs = docs
        
        # 2ë‹¨ê³„: ê° ë¬¸ì„œ ì²˜ë¦¬
        print("\nğŸ“ 2ë‹¨ê³„: í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì¤‘...")
        for idx, doc in enumerate(unique_docs, 1):
            print(f" Â  ì²˜ë¦¬ ì¤‘... [{idx}/{len(unique_docs)}]", end='\r')
            
            result = {
                "doc": doc,
                "is_duplicate": False,
                "key_sentences": [],
                "relevance_summary": ""
            }
            
            # í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
            if extract_key_sentences:
                key_info = await self._extract_key_info_llm(doc.page_content, user_query) # ğŸŒŸ await ì¶”ê°€
                result["key_sentences"] = key_info.get("key_sentences", [])
                result["relevance_summary"] = key_info.get("relevance_summary", "")
            
            processed_docs.append(result)
        
        print(f"\n Â  âœ… í•µì‹¬ ì¶”ì¶œ ì™„ë£Œ: {len(processed_docs)}ê°œ ë¬¸ì„œ")
        
        return processed_docs
    
    # ğŸŒŸ ë©”ì„œë“œ ì •ì˜: async ì¶”ê°€ ë° ë‚´ë¶€ await ì²˜ë¦¬
    async def _remove_duplicates_llm(self, docs: List[Document]) -> List[Document]:
        """LLM ê¸°ë°˜ ì¤‘ë³µ ë¬¸ì„œ ì œê±° (ë¹„ë™ê¸°)"""
        
        if len(docs) <= 1:
            return docs
        
        unique_docs = [docs[0]]  # ì²« ë²ˆì§¸ëŠ” í•­ìƒ í¬í•¨
        
        for idx, new_doc in enumerate(docs[1:], 2):
            # ê¸°ì¡´ ë¬¸ì„œë“¤ê³¼ ë¹„êµ
            is_duplicate = await self._check_duplicate_with_llm(new_doc, unique_docs) # ğŸŒŸ await ì¶”ê°€
            
            if not is_duplicate:
                unique_docs.append(new_doc)
        
        return unique_docs
    
    # ğŸŒŸ ë©”ì„œë“œ ì •ì˜: async ì¶”ê°€ ë° ë‚´ë¶€ await ì²˜ë¦¬
    async def _check_duplicate_with_llm(self, new_doc: Document, existing_docs: List[Document]) -> bool:
        """ìƒˆ ë¬¸ì„œê°€ ê¸°ì¡´ ë¬¸ì„œë“¤ê³¼ ì¤‘ë³µë˜ëŠ”ì§€ LLMìœ¼ë¡œ íŒë‹¨ (ë¹„ë™ê¸°)"""
        
        # ... (í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë¡œì§ ìœ ì§€) ...
        existing_summaries = []
        for doc in existing_docs[-3:]:
            metadata = doc.metadata
            summary = f"íŒŒì¼: {metadata.get('file', '?')}, ì„¹ì…˜: {metadata.get('section', '?')}"
            existing_summaries.append(summary)
        
        new_metadata = new_doc.metadata
        new_summary = f"íŒŒì¼: {new_metadata.get('file', '?')}, ì„¹ì…˜: {new_metadata.get('section', '?')}"
        
        prompt = f"""
ê¸°ì¡´ ë¬¸ì„œë“¤ (ìµœê·¼ 3ê°œ):
{chr(10).join(f"- {s}" for s in existing_summaries)}

ìƒˆ ë¬¸ì„œ:
{new_summary}

íŒë‹¨: ìƒˆ ë¬¸ì„œê°€ ê¸°ì¡´ ë¬¸ì„œë“¤ê³¼ **ê°™ì€ ë‚´ìš©**ì¸ê°€?

íŒë‹¨ ê¸°ì¤€:
- ê°™ì€ íŒŒì¼ì˜ ê°™ì€ ì„¹ì…˜ â†’ ì¤‘ë³µ
- ê°™ì€ íŒŒì¼ì˜ ë‹¤ë¥¸ ì„¹ì…˜ â†’ ë¹„ì¤‘ë³µ
- ë‹¤ë¥¸ íŒŒì¼ â†’ ë¹„ì¤‘ë³µ

JSON ì¶œë ¥ë§Œ:
{{"is_duplicate": true/false}}
"""
        
        try:
            # ğŸŒŸ LLM í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ì „í™˜ (cl.make_async ì‚¬ìš©)
            response = await cl.make_async(call_llm)(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=100
            )
            
            # JSON íŒŒì‹±
            result = self._parse_json(response, {"is_duplicate": False})
            return result.get("is_duplicate", False)
        
        except Exception as e:
            print(f"\nâš ï¸ ì¤‘ë³µ íŒë‹¨ ì‹¤íŒ¨: {e} (ë¹„ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼)")
            return False
    
    # ğŸŒŸ ë©”ì„œë“œ ì •ì˜: async ì¶”ê°€ ë° ë‚´ë¶€ await ì²˜ë¦¬
    async def _extract_key_info_llm(self, content: str, user_query: str) -> Dict[str, Any]:
        """LLMìœ¼ë¡œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ (ë¹„ë™ê¸°)"""
        
        prompt = f"""
ì‚¬ìš©ìê°€ ë‹¤ìŒ ì‚¬ê³ ë¥¼ ì¡°ì‚¬ ì¤‘ì…ë‹ˆë‹¤:

{user_query}

ë¬¸ì„œ ë‚´ìš©:
{content}

ì„ë¬´:
1. ì´ ë¬¸ì„œê°€ ì‚¬ê³ ì™€ ì–´ë–»ê²Œ ê´€ë ¨ë˜ëŠ”ì§€ **í•œ ë¬¸ì¥**ìœ¼ë¡œ ìš”ì•½
2. ì‚¬ê³  ì˜ˆë°©/ëŒ€ì‘ì— ë„ì›€ë˜ëŠ” **í•µì‹¬ ë¬¸ì¥ ìµœëŒ€ 3ê°œ** ì¶”ì¶œ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)

JSON ì¶œë ¥:
{{
    "relevance_summary": "ì´ ë¬¸ì„œëŠ” ì² ê·¼ ì‘ì—… ì‹œ ì‘ì—…ë°œíŒ ì„¤ì¹˜ ê¸°ì¤€ì„ ê·œì •í•¨",
    "key_sentences": [
        "ì‘ì—…ë°œíŒì€ ê²¬ê³ í•œ êµ¬ì¡°ë¡œ ì„¤ì¹˜ë˜ì–´ì•¼ í•œë‹¤.",
        "ë†’ì´ 2m ì´ìƒ ì‘ì—… ì‹œ ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í•  ê²ƒ.",
        "ì‘ì—…ë°œíŒ í­ì€ ìµœì†Œ 40cm ì´ìƒ í™•ë³´í•  ê²ƒ."
    ]
}}

ê·œì¹™:
- relevance_summary: ë°˜ë“œì‹œ í•œ ë¬¸ì¥
- key_sentences: ì›ë¬¸ì—ì„œ ì •í™•íˆ ì¶”ì¶œ, ìµœëŒ€ 3ê°œ
- ê´€ë ¨ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´
"""
        
        try:
            # ğŸŒŸ LLM í˜¸ì¶œì„ ë¹„ë™ê¸°ë¡œ ì „í™˜ (cl.make_async ì‚¬ìš©)
            response = await cl.make_async(call_llm)(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=800
            )
            
            result = self._parse_json(response, {
                "relevance_summary": "ê´€ë ¨ ì •ë³´ í¬í•¨",
                "key_sentences": []
            })
            
            return result
        
        except Exception as e:
            print(f"\nâš ï¸ í•µì‹¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "relevance_summary": "ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨",
                "key_sentences": []
            }
    
    def _parse_json(self, text: str, default: dict) -> dict:
        """LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ (ë¡œì§ ìœ ì§€)"""
        
        if not text:
            return default
        
        # 1ì°¨: ì „ì²´ íŒŒì‹±
        try:
            return json.loads(text)
        except:
            pass
        
        # 2ì°¨: { } ì¶”ì¶œ
        try:
            start = text.index("{")
            end = text.rindex("}") + 1
            json_str = text[start:end]
            return json.loads(json_str)
        except:
            pass
        
        return default
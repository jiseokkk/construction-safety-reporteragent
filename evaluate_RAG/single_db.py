import os
import sys
import pandas as pd
from tqdm import tqdm
from typing import List, Dict
import warnings
warnings.filterwarnings('ignore')

# retriever.py import
sys.path.append('/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent')

from core.retriever import SingleDBHybridRetriever

def calculate_hit_at_k(retrieved_docs: List, ground_truth_chunk: str, k: int = 5) -> int:
    """
    Retrieved documents ì¤‘ ground truth chunkê°€ ìˆëŠ”ì§€ í™•ì¸
    
    Args:
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        ground_truth_chunk: ì •ë‹µ chunk í…ìŠ¤íŠ¸
        k: top-k ë¬¸ì„œ í™•ì¸
    
    Returns:
        1 if hit, 0 if miss
    """
    for doc in retrieved_docs[:k]:
        # ì •ë‹µ chunkê°€ ê²€ìƒ‰ëœ ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if ground_truth_chunk.strip() in doc.page_content:
            return 1
    return 0


def calculate_mrr(retrieved_docs: List, ground_truth_chunk: str) -> float:
    """
    Mean Reciprocal Rank ê³„ì‚°
    
    Args:
        retrieved_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        ground_truth_chunk: ì •ë‹µ chunk í…ìŠ¤íŠ¸
    
    Returns:
        reciprocal rank (0 if not found)
    """
    for idx, doc in enumerate(retrieved_docs, 1):
        if ground_truth_chunk.strip() in doc.page_content:
            return 1.0 / idx
    return 0.0


def evaluate_retrieval(
    db_dir: str,
    eval_data_path: str,
    top_k: int = 5,
    alpha: float = 0.3,
    reranker_model: str = "BAAI/bge-reranker-v2-m3",
    output_path: str = None
):
    """
    RAG Retrieval ì„±ëŠ¥ í‰ê°€
    
    Args:
        db_dir: FAISS DB ê²½ë¡œ
        eval_data_path: í‰ê°€ ë°ì´í„° ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
        top_k: retriever top-k
        alpha: hybrid retrieval alpha (dense weight)
        reranker_model: reranker ëª¨ë¸ëª…
        output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ì €ì¥ ì•ˆí•¨)
    """
    print("=" * 80)
    print("RAG Retrieval í‰ê°€ ì‹œì‘")
    print("=" * 80)
    print(f"ğŸ“‚ DB ê²½ë¡œ: {db_dir}")
    print(f"ğŸ“„ í‰ê°€ ë°ì´í„°: {eval_data_path}")
    print(f"âš™ï¸  ì„¤ì •: top_k={top_k}, alpha={alpha}, reranker={reranker_model}")
    print("=" * 80)
    
    # 1. Retriever ì´ˆê¸°í™”
    print("\nğŸ”§ Retriever ì´ˆê¸°í™” ì¤‘...")
    retriever = SingleDBHybridRetriever(
        db_dir=db_dir,
        top_k=top_k,
        alpha=alpha,
        reranker_model=reranker_model
    )
    
    # 2. í‰ê°€ ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“Š í‰ê°€ ë°ì´í„° ë¡œë“œ ì¤‘...")
    eval_df = pd.read_excel(eval_data_path)
    print(f"ì´ {len(eval_df)}ê°œì˜ í‰ê°€ ì¿¼ë¦¬")
    
    # 3. í‰ê°€ ìˆ˜í–‰
    print(f"\nğŸ” ê²€ìƒ‰ ë° í‰ê°€ ìˆ˜í–‰ ì¤‘...")
    results = []
    hit_at_1_sum = 0
    hit_at_3_sum = 0
    hit_at_5_sum = 0
    mrr_sum = 0
    
    for idx, row in tqdm(eval_df.iterrows(), total=len(eval_df), desc="í‰ê°€ ì§„í–‰"):
        question = row['question ']  # ê³µë°± ì£¼ì˜
        ground_truth_chunk = row['chunk']
        doctitle = row['doctitle']
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        try:
            retrieved_docs = retriever.retrieve(question)
            
            # Hit@K ê³„ì‚°
            hit_at_1 = calculate_hit_at_k(retrieved_docs, ground_truth_chunk, k=1)
            hit_at_3 = calculate_hit_at_k(retrieved_docs, ground_truth_chunk, k=3)
            hit_at_5 = calculate_hit_at_k(retrieved_docs, ground_truth_chunk, k=5)
            
            # MRR ê³„ì‚°
            mrr = calculate_mrr(retrieved_docs, ground_truth_chunk)
            
            # ëˆ„ì 
            hit_at_1_sum += hit_at_1
            hit_at_3_sum += hit_at_3
            hit_at_5_sum += hit_at_5
            mrr_sum += mrr
            
            # ê°œë³„ ê²°ê³¼ ì €ì¥
            results.append({
                'index': idx,
                'doctitle': doctitle,
                'question': question,
                'ground_truth_chunk': ground_truth_chunk,
                'hit@1': hit_at_1,
                'hit@3': hit_at_3,
                'hit@5': hit_at_5,
                'mrr': mrr,
                'retrieved_docs_count': len(retrieved_docs)
            })
            
        except Exception as e:
            print(f"\nâš ï¸  Query {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            results.append({
                'index': idx,
                'doctitle': doctitle,
                'question': question,
                'ground_truth_chunk': ground_truth_chunk,
                'hit@1': 0,
                'hit@3': 0,
                'hit@5': 0,
                'mrr': 0,
                'retrieved_docs_count': 0,
                'error': str(e)
            })
    
    # 4. ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
    num_queries = len(eval_df)
    avg_hit_at_1 = hit_at_1_sum / num_queries
    avg_hit_at_3 = hit_at_3_sum / num_queries
    avg_hit_at_5 = hit_at_5_sum / num_queries
    avg_mrr = mrr_sum / num_queries
    
    # 5. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“ˆ í‰ê°€ ê²°ê³¼")
    print("=" * 80)
    print(f"ì´ ì¿¼ë¦¬ ìˆ˜: {num_queries}")
    print(f"\nã€ Hit Rate ã€‘")
    print(f"  Hit@1: {avg_hit_at_1:.4f} ({avg_hit_at_1*100:.2f}%)")
    print(f"  Hit@3: {avg_hit_at_3:.4f} ({avg_hit_at_3*100:.2f}%)")
    print(f"  Hit@5: {avg_hit_at_5:.4f} ({avg_hit_at_5*100:.2f}%)")
    print(f"\nã€ MRR (Mean Reciprocal Rank) ã€‘")
    print(f"  MRR: {avg_mrr:.4f}")
    print("=" * 80)
    
    # 6. ê²°ê³¼ ì €ì¥
    if output_path:
        results_df = pd.DataFrame(results)
        results_df.to_excel(output_path, index=False)
        print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥: {output_path}")
        
        # ìš”ì•½ ì €ì¥
        summary_path = output_path.replace('.xlsx', '_summary.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("RAG Retrieval í‰ê°€ ê²°ê³¼ ìš”ì•½\n")
            f.write("=" * 80 + "\n")
            f.write(f"DB ê²½ë¡œ: {db_dir}\n")
            f.write(f"í‰ê°€ ë°ì´í„°: {eval_data_path}\n")
            f.write(f"ì„¤ì •: top_k={top_k}, alpha={alpha}, reranker={reranker_model}\n")
            f.write("=" * 80 + "\n")
            f.write(f"ì´ ì¿¼ë¦¬ ìˆ˜: {num_queries}\n\n")
            f.write("ã€ Hit Rate ã€‘\n")
            f.write(f"  Hit@1: {avg_hit_at_1:.4f} ({avg_hit_at_1*100:.2f}%)\n")
            f.write(f"  Hit@3: {avg_hit_at_3:.4f} ({avg_hit_at_3*100:.2f}%)\n")
            f.write(f"  Hit@5: {avg_hit_at_5:.4f} ({avg_hit_at_5*100:.2f}%)\n\n")
            f.write("ã€ MRR (Mean Reciprocal Rank) ã€‘\n")
            f.write(f"  MRR: {avg_mrr:.4f}\n")
            f.write("=" * 80 + "\n")
        print(f"ğŸ“„ ìš”ì•½ ê²°ê³¼ ì €ì¥: {summary_path}")
    
    return {
        'hit@1': avg_hit_at_1,
        'hit@3': avg_hit_at_3,
        'hit@5': avg_hit_at_5,
        'mrr': avg_mrr,
        'results': results
    }


def compare_retrieval_configs(
    db_dir: str,
    eval_data_path: str,
    configs: List[Dict],
    output_dir: str = "/home/claude/eval_results"
):
    """
    ì—¬ëŸ¬ retrieval ì„¤ì • ë¹„êµ
    
    Args:
        db_dir: FAISS DB ê²½ë¡œ
        eval_data_path: í‰ê°€ ë°ì´í„° ê²½ë¡œ
        configs: ë¹„êµí•  ì„¤ì • ë¦¬ìŠ¤íŠ¸ (ê°ê° dict with top_k, alpha, reranker_model)
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    os.makedirs(output_dir, exist_ok=True)
    
    comparison_results = []
    
    for config_idx, config in enumerate(configs, 1):
        print(f"\n{'='*80}")
        print(f"ì„¤ì • {config_idx}/{len(configs)} í‰ê°€")
        print(f"{'='*80}")
        
        output_path = os.path.join(
            output_dir, 
            f"config_{config_idx}_k{config['top_k']}_a{config['alpha']}.xlsx"
        )
        
        result = evaluate_retrieval(
            db_dir=db_dir,
            eval_data_path=eval_data_path,
            top_k=config.get('top_k', 5),
            alpha=config.get('alpha', 0.3),
            reranker_model=config.get('reranker_model', 'BAAI/bge-reranker-v2-m3'),
            output_path=output_path
        )
        
        comparison_results.append({
            'config_name': f"config_{config_idx}",
            'top_k': config.get('top_k', 5),
            'alpha': config.get('alpha', 0.3),
            'reranker': config.get('reranker_model', 'BAAI/bge-reranker-v2-m3'),
            'hit@1': result['hit@1'],
            'hit@3': result['hit@3'],
            'hit@5': result['hit@5'],
            'mrr': result['mrr']
        })
    
    # ë¹„êµ ê²°ê³¼ ì €ì¥
    comparison_df = pd.DataFrame(comparison_results)
    comparison_path = os.path.join(output_dir, 'comparison_summary.xlsx')
    comparison_df.to_excel(comparison_path, index=False)
    
    print(f"\n{'='*80}")
    print("ğŸ† ì „ì²´ ë¹„êµ ê²°ê³¼")
    print(f"{'='*80}")
    print(comparison_df.to_string(index=False))
    print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_path}")
    
    return comparison_results


if __name__ == "__main__":
    # ë‹¨ì¼ DB í‰ê°€
    DB_DIR = "/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/DB2/content_db"
    EVAL_DATA_PATH = "/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/evaluate_RAG/capstone_retrieval_eval_data.xlsx"
    OUTPUT_PATH = "/home/user/Desktop/jiseok/capstone/RAG/construction-safety-agent/evaluate_RAG/capstone_retrieval_eval_data.xlsx"
    
    # í‰ê°€ ì‹¤í–‰
    result = evaluate_retrieval(
        db_dir=DB_DIR,
        eval_data_path=EVAL_DATA_PATH,
        top_k=5,
        alpha=0.3,
        reranker_model="BAAI/bge-reranker-v2-m3",
        output_path=OUTPUT_PATH
    )
    
    # ì—¬ëŸ¬ ì„¤ì • ë¹„êµ (ì˜µì…˜)
    # configs = [
    #     {'top_k': 3, 'alpha': 0.3},
    #     {'top_k': 5, 'alpha': 0.3},
    #     {'top_k': 10, 'alpha': 0.3},
    #     {'top_k': 5, 'alpha': 0.5},
    #     {'top_k': 5, 'alpha': 0.7},
    # ]
    # compare_retrieval_configs(DB_DIR, EVAL_DATA_PATH, configs)